{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a01f9ce2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6666666666666666\n",
      "Coefficients (Beta values): [0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define the data\n",
    "data = {\n",
    "    \"Person No.\": [1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6, 7, 7, 7, 8, 8, 8, 9, 9, 9, 10, 10, 10],\n",
    "    \"Mode\": [1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3],\n",
    "    \"Time\": [15, 30, 20, 10, 15, 10, 5, 20, 7, 25, 60, 20, 10, 12, 10, 20, 35, 23, 25, 90, 20, 15, 30, 20, 10, 15, 10, 25, 20, 25],\n",
    "    \"Cost\": [7.5, 0.0, 3.5, 7.5, 0.0, 3.5, 7.5, 0.0, 3.5, 25.0, 0.0, 5.0, 7.5, 0.0, 3.5, 7.5, 0.0, 5.5, 27.5, 0.0, 7.5, 7.5, 0.0, 3.5, 7.5, 0.0, 3.5, 8.0, 0.0, 5.0],\n",
    "    \"Choice\": [1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Prepare feature matrix X and target vector y\n",
    "X = df[['Time', 'Cost']].values\n",
    "y = df['Choice'].values\n",
    "\n",
    "# One-hot encode the target variable y\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_encoded = encoder.fit_transform(df[['Choice']])\n",
    "\n",
    "# Define the softmax function\n",
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # Subtract max for numerical stability\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "# Define the negative log-likelihood function for multinomial logistic regression\n",
    "def neg_log_likelihood(params, X, y):\n",
    "    logits = np.dot(X, params.reshape(-1, 1))\n",
    "    logits = logits - np.max(logits, axis=1, keepdims=True)  # For numerical stability\n",
    "    exp_logits = np.exp(logits)\n",
    "    probs = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
    "    log_likelihood = -np.sum(y * np.log(probs))\n",
    "    return log_likelihood\n",
    "\n",
    "# Define the prediction function\n",
    "def predict(params, X):\n",
    "    logits = np.dot(X, params.reshape(-1, 1))\n",
    "    probs = softmax(logits)\n",
    "    return np.argmax(probs, axis=1)\n",
    "\n",
    "# Initialize parameters (coefficients only, no intercepts)\n",
    "initial_params = np.zeros(X.shape[1])\n",
    "\n",
    "# Minimize the negative log-likelihood\n",
    "result = minimize(neg_log_likelihood, initial_params, args=(X, y_encoded), method='BFGS')\n",
    "optimal_params = result.x\n",
    "\n",
    "# Predict class labels using the optimized parameters\n",
    "y_pred = predict(optimal_params, X)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Output the optimized coefficients\n",
    "print(\"Coefficients (Beta values):\", optimal_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8b5205",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
