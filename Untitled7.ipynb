{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d46e348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-likelihood params: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "Beta matrix: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "loop of ufunc does not support argument 0 of type float which has no callable exp method",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: 'float' object has no attribute 'exp'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 67\u001b[0m\n\u001b[0;32m     64\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(y))\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# Fit the model\u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m beta \u001b[38;5;241m=\u001b[39m fit_multinomial_logistic_regression(X\u001b[38;5;241m.\u001b[39mvalues, y\u001b[38;5;241m.\u001b[39mvalues, num_classes)\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIntercept and Coefficients:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(beta)\n",
      "Cell \u001b[1;32mIn[6], line 57\u001b[0m, in \u001b[0;36mfit_multinomial_logistic_regression\u001b[1;34m(X, y, num_classes, max_iter)\u001b[0m\n\u001b[0;32m     55\u001b[0m num_samples, num_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m     56\u001b[0m initial_params \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((num_classes \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, num_features \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mravel()\n\u001b[1;32m---> 57\u001b[0m result \u001b[38;5;241m=\u001b[39m minimize(log_likelihood, initial_params, args\u001b[38;5;241m=\u001b[39m(X, y, num_classes),\n\u001b[0;32m     58\u001b[0m                   method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mL-BFGS-B\u001b[39m\u001b[38;5;124m'\u001b[39m, options\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaxiter\u001b[39m\u001b[38;5;124m'\u001b[39m: max_iter})\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Add zero coefficients for the reference class to the result\u001b[39;00m\n\u001b[0;32m     60\u001b[0m beta \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack([np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m, num_features \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)), result\u001b[38;5;241m.\u001b[39mx\u001b[38;5;241m.\u001b[39mreshape(num_classes \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, num_features \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)])\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_minimize.py:710\u001b[0m, in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    707\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[0;32m    708\u001b[0m                              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml-bfgs-b\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 710\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0;32m    711\u001b[0m                            callback\u001b[38;5;241m=\u001b[39mcallback, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m    712\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtnc\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    713\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[0;32m    714\u001b[0m                         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_lbfgsb_py.py:307\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[1;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    305\u001b[0m         iprint \u001b[38;5;241m=\u001b[39m disp\n\u001b[1;32m--> 307\u001b[0m sf \u001b[38;5;241m=\u001b[39m _prepare_scalar_function(fun, x0, jac\u001b[38;5;241m=\u001b[39mjac, args\u001b[38;5;241m=\u001b[39margs, epsilon\u001b[38;5;241m=\u001b[39meps,\n\u001b[0;32m    308\u001b[0m                               bounds\u001b[38;5;241m=\u001b[39mnew_bounds,\n\u001b[0;32m    309\u001b[0m                               finite_diff_rel_step\u001b[38;5;241m=\u001b[39mfinite_diff_rel_step)\n\u001b[0;32m    311\u001b[0m func_and_grad \u001b[38;5;241m=\u001b[39m sf\u001b[38;5;241m.\u001b[39mfun_and_grad\n\u001b[0;32m    313\u001b[0m fortran_int \u001b[38;5;241m=\u001b[39m _lbfgsb\u001b[38;5;241m.\u001b[39mtypes\u001b[38;5;241m.\u001b[39mintvar\u001b[38;5;241m.\u001b[39mdtype\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_optimize.py:383\u001b[0m, in \u001b[0;36m_prepare_scalar_function\u001b[1;34m(fun, x0, jac, args, bounds, epsilon, finite_diff_rel_step, hess)\u001b[0m\n\u001b[0;32m    379\u001b[0m     bounds \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf, np\u001b[38;5;241m.\u001b[39minf)\n\u001b[0;32m    381\u001b[0m \u001b[38;5;66;03m# ScalarFunction caches. Reuse of fun(x) during grad\u001b[39;00m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;66;03m# calculation reduces overall function evaluations.\u001b[39;00m\n\u001b[1;32m--> 383\u001b[0m sf \u001b[38;5;241m=\u001b[39m ScalarFunction(fun, x0, args, grad, hess,\n\u001b[0;32m    384\u001b[0m                     finite_diff_rel_step, bounds, epsilon\u001b[38;5;241m=\u001b[39mepsilon)\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sf\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:158\u001b[0m, in \u001b[0;36mScalarFunction.__init__\u001b[1;34m(self, fun, x0, args, grad, hess, finite_diff_rel_step, finite_diff_bounds, epsilon)\u001b[0m\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m fun_wrapped(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx)\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun_impl \u001b[38;5;241m=\u001b[39m update_fun\n\u001b[1;32m--> 158\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun()\n\u001b[0;32m    160\u001b[0m \u001b[38;5;66;03m# Gradient evaluation\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(grad):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:251\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated:\n\u001b[1;32m--> 251\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun_impl()\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:155\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[1;34m()\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_fun\u001b[39m():\n\u001b[1;32m--> 155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m fun_wrapped(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:137\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnfev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m fx \u001b[38;5;241m=\u001b[39m fun(np\u001b[38;5;241m.\u001b[39mcopy(x), \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    138\u001b[0m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
      "Cell \u001b[1;32mIn[6], line 47\u001b[0m, in \u001b[0;36mlog_likelihood\u001b[1;34m(params, X, y, num_classes)\u001b[0m\n\u001b[0;32m     45\u001b[0m X_intercept \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack([np\u001b[38;5;241m.\u001b[39mones((num_samples, \u001b[38;5;241m1\u001b[39m)), X])  \u001b[38;5;66;03m# Add intercept term\u001b[39;00m\n\u001b[0;32m     46\u001b[0m logits \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(X_intercept, beta\u001b[38;5;241m.\u001b[39mT)\n\u001b[1;32m---> 47\u001b[0m probabilities \u001b[38;5;241m=\u001b[39m softmax(logits)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProbabilities:\u001b[39m\u001b[38;5;124m\"\u001b[39m, probabilities)  \u001b[38;5;66;03m# Debug statement\u001b[39;00m\n\u001b[0;32m     49\u001b[0m ll \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(np\u001b[38;5;241m.\u001b[39mlog(probabilities[np\u001b[38;5;241m.\u001b[39marange(num_samples), y]))\n",
      "Cell \u001b[1;32mIn[6], line 34\u001b[0m, in \u001b[0;36msoftmax\u001b[1;34m(z)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msoftmax\u001b[39m(z):\n\u001b[1;32m---> 34\u001b[0m     exp_z \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(z)\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExponentiated values (exp_z):\u001b[39m\u001b[38;5;124m\"\u001b[39m, exp_z)  \u001b[38;5;66;03m# Debug statement\u001b[39;00m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m exp_z \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msum(exp_z, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mTypeError\u001b[0m: loop of ufunc does not support argument 0 of type float which has no callable exp method"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Load the dataset\n",
    "link = \"https://stats.idre.ucla.edu/stat/data/hsb2.csv\"\n",
    "webUrl = urllib.request.urlopen(link)\n",
    "hsb2 = pd.read_csv(webUrl)\n",
    "\n",
    "# Convert categorical columns to category dtype\n",
    "hsb2['female'] = hsb2['female'].astype('category')\n",
    "hsb2['race'] = hsb2['race'].astype('category')\n",
    "hsb2['ses'] = hsb2['ses'].astype('category')\n",
    "\n",
    "# Convert categorical columns to dummy variables\n",
    "race = pd.get_dummies(hsb2['race'], drop_first=True, prefix='race')\n",
    "ses = pd.get_dummies(hsb2['ses'], drop_first=True, prefix='ses')\n",
    "female = pd.get_dummies(hsb2['female'], drop_first=True, prefix='female')\n",
    "\n",
    "# Drop original categorical columns and add dummy columns\n",
    "hsb3 = hsb2.drop(['race', 'ses', 'female'], axis=1)\n",
    "hsb3 = pd.concat([hsb3, race, ses, female], axis=1)\n",
    "\n",
    "# Define X and y\n",
    "y = hsb3['prog']\n",
    "X = hsb3.drop(['prog', 'id'], axis=1)\n",
    "\n",
    "# Debug print statements to check data types and contents\n",
    "\n",
    "\n",
    "# Define the softmax function\n",
    "def softmax(z):\n",
    "    exp_z = np.exp(z)\n",
    "    print(\"Exponentiated values (exp_z):\", exp_z)  # Debug statement\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "# Define the log-likelihood function\n",
    "def log_likelihood(params, X, y, num_classes):\n",
    "    print(\"Log-likelihood params:\", params)  # Debug statement\n",
    "    num_samples, num_features = X.shape\n",
    "    params = params.reshape((num_classes - 1, num_features + 1))\n",
    "    beta = np.vstack([np.zeros((1, num_features + 1)), params])\n",
    "    print(\"Beta matrix:\", beta)  # Debug statement\n",
    "    X_intercept = np.hstack([np.ones((num_samples, 1)), X])  # Add intercept term\n",
    "    logits = np.dot(X_intercept, beta.T)\n",
    "    probabilities = softmax(logits)\n",
    "    print(\"Probabilities:\", probabilities)  # Debug statement\n",
    "    ll = np.sum(np.log(probabilities[np.arange(num_samples), y]))\n",
    "    print(\"Log-likelihood:\", ll)  # Debug statement\n",
    "    return -ll  # We negate because we will be minimizing\n",
    "\n",
    "# Fit the model\n",
    "def fit_multinomial_logistic_regression(X, y, num_classes, max_iter=100):\n",
    "    num_samples, num_features = X.shape\n",
    "    initial_params = np.zeros((num_classes - 1, num_features + 1)).ravel()\n",
    "    result = minimize(log_likelihood, initial_params, args=(X, y, num_classes),\n",
    "                      method='L-BFGS-B', options={'maxiter': max_iter})\n",
    "    # Add zero coefficients for the reference class to the result\n",
    "    beta = np.vstack([np.zeros((1, num_features + 1)), result.x.reshape(num_classes - 1, num_features + 1)])\n",
    "    return beta\n",
    "\n",
    "# Define the number of classes based on your dataset\n",
    "num_classes = len(np.unique(y))\n",
    "\n",
    "# Fit the model\n",
    "beta = fit_multinomial_logistic_regression(X.values, y.values, num_classes)\n",
    "\n",
    "print(\"Intercept and Coefficients:\")\n",
    "print(beta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4183a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input to softmax (z): [[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "Type of input (z): <class 'numpy.ndarray'>\n",
      "Shape of input (z): (3, 3)\n",
      "Exponential of z: [[1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]]\n",
      "Sum of exponentials of z: [[3.]\n",
      " [3.]\n",
      " [3.]]\n",
      "Softmax output: [[0.33333333 0.33333333 0.33333333]\n",
      " [0.33333333 0.33333333 0.33333333]\n",
      " [0.33333333 0.33333333 0.33333333]]\n",
      "Final softmax output: [[0.33333333 0.33333333 0.33333333]\n",
      " [0.33333333 0.33333333 0.33333333]\n",
      " [0.33333333 0.33333333 0.33333333]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(z):\n",
    "    print(f\"Input to softmax (z): {z}\")\n",
    "    print(f\"Type of input (z): {type(z)}\")\n",
    "    print(f\"Shape of input (z): {np.shape(z)}\")\n",
    "    \n",
    "    # If z is not an array, convert it to an array\n",
    "    if isinstance(z, (int, float)):\n",
    "        z = np.array([z])\n",
    "    \n",
    "    exp_z = np.exp(z)\n",
    "    print(f\"Exponential of z: {exp_z}\")\n",
    "    \n",
    "    sum_exp_z = np.sum(exp_z, axis=1, keepdims=True)\n",
    "    print(f\"Sum of exponentials of z: {sum_exp_z}\")\n",
    "    \n",
    "    softmax_output = exp_z / sum_exp_z\n",
    "    print(f\"Softmax output: {softmax_output}\")\n",
    "    \n",
    "    return softmax_output\n",
    "\n",
    "# Example usage\n",
    "logits = np.array([[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]])\n",
    "softmax_output = softmax(logits)\n",
    "print(f\"Final softmax output: {softmax_output}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efec035a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
