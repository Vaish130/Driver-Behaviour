{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e90a4746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import pandas as pd\n",
    "import requests\n",
    "import io\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from matplotlib import pyplot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b292bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "98220763",
   "metadata": {},
   "outputs": [],
   "source": [
    "hsb2 = pd.read_csv(\"C:\\\\Users\\\\Admin\\\\Desktop\\\\Vaishnavi\\\\Auto\\\\NL.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ad8134f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mode</th>\n",
       "      <th>Time</th>\n",
       "      <th>Cost</th>\n",
       "      <th>Hinc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>7.5</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>2.5</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>3.5</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>7.5</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>2.5</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Mode  Time  Cost  Hinc\n",
       "0     1    15   7.5     9\n",
       "1     0    30   2.5     9\n",
       "2     0    20   3.5     9\n",
       "3     0    10   7.5     7\n",
       "4     1    15   2.5     7"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hsb2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "464badf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mode      int64\n",
       "Time      int64\n",
       "Cost    float64\n",
       "Hinc      int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hsb2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e86c0a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hsb2[\"Cost\"] = hsb2[\"Cost\"].astype('int64')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "822346ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mode    int64\n",
       "Time    int64\n",
       "Cost    int64\n",
       "Hinc    int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hsb2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0d84c9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hsb3 = hsb2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b420e6b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mode</th>\n",
       "      <th>Time</th>\n",
       "      <th>Cost</th>\n",
       "      <th>Hinc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Mode  Time  Cost  Hinc\n",
       "0     1    15     7     9\n",
       "1     0    30     2     9\n",
       "2     0    20     3     9\n",
       "3     0    10     7     7\n",
       "4     1    15     2     7"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hsb3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48f3219",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ffc4cc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = hsb3['Mode']\n",
    "X = hsb3.drop(['Mode'],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "59ee1d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6666666666666666\n",
      "Intercepts and Coefficients: [[ 0.01926213  0.01483844 -0.01439499]\n",
      " [-0.01975957 -0.01490095  0.01433571]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define the softmax function\n",
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # Subtract max for numerical stability\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "# Define the negative log-likelihood function for multinomial logistic regression\n",
    "def neg_log_likelihood(params, X, y, num_classes):\n",
    "    num_features = X.shape[1]\n",
    "    params = params.reshape((num_classes, num_features))\n",
    "    logits = np.dot(X, params.T)\n",
    "    probs = softmax(logits)\n",
    "    log_likelihood = np.sum(y * np.log(probs))\n",
    "    return -log_likelihood\n",
    "\n",
    "# Define the prediction function\n",
    "def predict(params, X, num_classes):\n",
    "    num_features = X.shape[1]\n",
    "    params = params.reshape((num_classes, num_features))\n",
    "    logits = np.dot(X, params.T)\n",
    "    probs = softmax(logits)\n",
    "    return np.argmax(probs, axis=1)\n",
    "\n",
    "# Assuming X and y are already defined\n",
    "# Convert y to numpy array if it is a pandas Series\n",
    "if isinstance(y, pd.Series):\n",
    "    y = y.to_numpy()\n",
    "\n",
    "# Add a small constant to X to avoid log(0) or log of negative values\n",
    "small_constant = 1e-10\n",
    "X_transformed = X + small_constant\n",
    "\n",
    "# One-hot encode the target variable y\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_encoded = encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Get the number of classes and features\n",
    "num_classes = y_encoded.shape[1]\n",
    "num_features = X_transformed.shape[1]\n",
    "\n",
    "# Initialize parameters\n",
    "initial_params = np.zeros((num_classes, num_features))\n",
    "\n",
    "# Minimize the negative log-likelihood\n",
    "result = minimize(neg_log_likelihood, initial_params.ravel(), args=(X_transformed, y_encoded, num_classes), method='BFGS')\n",
    "optimal_params = result.x\n",
    "\n",
    "# Predict class labels using the optimized parameters\n",
    "y_pred = predict(optimal_params, X_transformed, num_classes)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Reshape the optimized parameters to get the intercepts and coefficients\n",
    "optimal_params = optimal_params.reshape((num_classes, num_features))\n",
    "print(\"Intercepts and Coefficients:\", optimal_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d82bedcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6666666666666666\n",
      "Intercepts: [-0.44745913  0.4474571 ]\n",
      "Coefficients: [[ 0.02412407  0.02633354  0.02643919]\n",
      " [-0.02466459 -0.02640149 -0.02650095]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define the softmax function\n",
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # Subtract max for numerical stability\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "# Define the negative log-likelihood function for multinomial logistic regression\n",
    "def neg_log_likelihood(params, X, y, num_classes, num_features):\n",
    "    intercepts = params[:num_classes]\n",
    "    coefficients = params[num_classes:].reshape((num_classes, num_features))\n",
    "    logits = np.dot(X, coefficients.T) + intercepts\n",
    "    probs = softmax(logits)\n",
    "    log_likelihood = np.sum(y * np.log(probs))\n",
    "    return -log_likelihood\n",
    "\n",
    "# Define the prediction function\n",
    "def predict(params, X, num_classes, num_features):\n",
    "    intercepts = params[:num_classes]\n",
    "    coefficients = params[num_classes:].reshape((num_classes, num_features))\n",
    "    logits = np.dot(X, coefficients.T) + intercepts\n",
    "    probs = softmax(logits)\n",
    "    return np.argmax(probs, axis=1)\n",
    "\n",
    "# Assuming X and y are already defined\n",
    "# Convert y to numpy array if it is a pandas Series\n",
    "if isinstance(y, pd.Series):\n",
    "    y = y.to_numpy()\n",
    "\n",
    "# Add a small constant to X to avoid log(0) or log of negative values\n",
    "small_constant = 1e-10\n",
    "X_transformed = X + small_constant\n",
    "\n",
    "# One-hot encode the target variable y\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_encoded = encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Get the number of classes and features\n",
    "num_classes = y_encoded.shape[1]\n",
    "num_features = X_transformed.shape[1]\n",
    "\n",
    "# Initialize parameters (intercepts and coefficients)\n",
    "initial_intercepts = np.zeros(num_classes)\n",
    "initial_coefficients = np.zeros((num_classes, num_features))\n",
    "initial_params = np.concatenate([initial_intercepts, initial_coefficients.ravel()])\n",
    "\n",
    "# Minimize the negative log-likelihood\n",
    "result = minimize(neg_log_likelihood, initial_params, args=(X_transformed, y_encoded, num_classes, num_features), method='BFGS')\n",
    "optimal_params = result.x\n",
    "\n",
    "# Predict class labels using the optimized parameters\n",
    "y_pred = predict(optimal_params, X_transformed, num_classes, num_features)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Extract intercepts and coefficients\n",
    "optimal_intercepts = optimal_params[:num_classes]\n",
    "optimal_coefficients = optimal_params[num_classes:].reshape((num_classes, num_features))\n",
    "\n",
    "print(\"Intercepts:\", optimal_intercepts)\n",
    "print(\"Coefficients:\", optimal_coefficients)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c04b546",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin, RegressorMixin\n",
    "\n",
    "class Multinomial_Regression(BaseEstimator, ClassifierMixin): \n",
    "    def __init__(self, X, y,params=None):     \n",
    "        if (params == None):\n",
    "            self.learningRate = 0.005                  # Learning Rate\n",
    "            self.max_epoch = 3000                      \n",
    "        else:\n",
    "            self.learningRate = params['LearningRate']\n",
    "            self.max_epoch = params['Epoch'] # Epochs\n",
    "        self.weight = np.array([[0.1,0.2,0.3],\n",
    "                               [0.1,0.2,0.3],\n",
    "                               [0.1,0.2,0.3],\n",
    "                               [0.1,0.2,0.3]])\n",
    "\n",
    "    def cost_derivate_gradient(self,n,Ti,Oi, X):\n",
    "        result = -(np.dot(X.T,(Ti - Oi)))/n   \n",
    "        return result \n",
    "\n",
    "    def function_cost_J(self,n,Ti,Oi):\n",
    "        result = -(np.sum(Ti * np.log(Oi)))/n \n",
    "        return result\n",
    "    def one_hot_encoding(self,Y):\n",
    "        OneHotEncoding = []\n",
    "        encoding = []\n",
    "        for i in range(len(Y)):\n",
    "            if(Y[i] == 0): encoding = np.array([1,0,0]) #Class 1, if y = 0\n",
    "            elif(Y[i] == 1): encoding = np.array([0,1,0]) #Class 2, if y = 1\n",
    "            elif(Y[i] == 2): encoding = np.array([0,0,1]) #Class 3, if y = 2\n",
    "\n",
    "            OneHotEncoding.append(encoding)\n",
    "        return OneHotEncoding\n",
    "    \n",
    "    def accuracy_graphic(self, answer_graph):\n",
    "        labels = 'Hits', 'Faults'\n",
    "        sizes = [96.5, 3.3]\n",
    "        explode = (0, 0.14)\n",
    "        fig1, ax1 = plt.subplots()\n",
    "        ax1.pie(answer_graph, explode=explode, colors=['green','red'], labels=labels, autopct='%1.1f%%',\n",
    "        shadow=True, startangle=90)\n",
    "        ax1.axis('equal')\n",
    "        plt.show()\n",
    "\n",
    "    def softmax(self,z):\n",
    "        soft = (np.exp(z).T / np.sum(np.exp(z),axis=1)).T \n",
    "        return soft\n",
    "    \n",
    "    def show_probability(self, arrayProbability):\n",
    "        print(\"Probability: [ Class 0 ,  Class 1 , Class 2 ]\")\n",
    "        \n",
    "        arrayTotal = []\n",
    "        for k in arrayProbability:\n",
    "            k[0] = \"%.3f\" % k[0]\n",
    "            k[1] = \"%.3f\" % k[1]\n",
    "            k[2] = \"%.3f\" % k[2]\n",
    "            arrayTotal.append(k)\n",
    "        for index, data in enumerate(arrayTotal):\n",
    "            prob0 = data[0] * 100\n",
    "            prob1 = data[1] * 100\n",
    "            prob2 = data[2] * 100\n",
    "            string = \" {}: {}%, {}%, {}%\".format(index, \"%.3f\" % prob0, \"%.3f\" % prob1, \"%.3f\" % prob2)\n",
    "            print(string)\n",
    "        \n",
    "    def predict(self, X,y):\n",
    "        acc_set = acc_vers = acc_virg = 0\n",
    "        v_resp = []\n",
    "        n = len(y)\n",
    "        Z = np.matmul(X, self.weight)\n",
    "        Oi = self.softmax(Z)\n",
    "        prevision = np.argmax(Oi,axis=1)\n",
    "        self.show_probability(Oi)\n",
    "        print(\"\")\n",
    "        procent = sum(prevision == y)/n\n",
    "        print(\" ID-Sample  | Class Classification |  Output |   Hoped output  \")  \n",
    "        for i in range(len(prevision)):\n",
    "            if(prevision[i] == 0): print(\" id :\",i,\"          | Iris-Setosa        |  Output:\",prevision[i],\"   |\",y[i])\n",
    "            elif(prevision[i] == 1): print(\" id :\",i,\"          | Iris-Versicolour   |  Output:\",prevision[i],\"   |\",y[i])\n",
    "            elif(prevision[i] == 2): print(\" id :\",i,\"          | Iris-Virginica     |  Output:\",prevision[i],\"   |\",y[i])\n",
    "                \n",
    "        for i in range(len(prevision)):\n",
    "            if((prevision[i] == y[i])and(prevision[i] == 0)):acc_set+=1\n",
    "            elif((prevision[i] == y[i])and(prevision[i] == 1)):acc_vers+=1\n",
    "            elif((prevision[i] == y[i])and(prevision[i] == 2)):acc_virg+=1\n",
    "               \n",
    "        correct = procent * 100\n",
    "        incorrect = 100 - correct\n",
    "        v_resp.append(correct)\n",
    "        v_resp.append(incorrect)\n",
    "        self.accuracy_graphic(v_resp)\n",
    "        return \"%.2f\"%(correct), acc_set, acc_vers, acc_virg\n",
    "    def show_err_graphic(self,v_epoch,v_error):\n",
    "        plt.figure(figsize=(9,4))\n",
    "        plt.plot(v_epoch, v_error, \"m-\")\n",
    "        plt.xlabel(\"Number of Epoch\")\n",
    "        plt.ylabel(\"Error\")\n",
    "        plt.title(\"Error Minimization\")\n",
    "        plt.show()\n",
    "\n",
    "    def fit(self,X,y):\n",
    "        v_epochs = []\n",
    "        totalError = []\n",
    "        epochCount = 0\n",
    "        n = len(X)\n",
    "        gradientE = []\n",
    "        while(epochCount < self.max_epoch):\n",
    "            Ti = self.one_hot_encoding(y)\n",
    "            Z = np.matmul(X,self.weight)\n",
    "            Oi = self.softmax(Z)\n",
    "            erro = self.function_cost_J(n,Ti,Oi)\n",
    "            gradient = self.cost_derivate_gradient(n,Ti,Oi,X)\n",
    "            self.weight = self.weight - self.learningRate * gradient\n",
    "            if(epochCount % 100 == 0):\n",
    "                totalError.append(erro)\n",
    "                gradientE.append(gradient)\n",
    "                v_epochs.append(epochCount)\n",
    "                print(\"Epoch \",epochCount,\" Total Error:\", \"%.4f\" % erro)\n",
    "            \n",
    "            epochCount += 1\n",
    "        \n",
    "        self.show_err_graphic(v_epochs,totalError)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c519c85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3526, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_15116\\3439214296.py\", line 3, in <module>\n",
      "    SoftmaxRegression.fit(X,y)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_15116\\412797928.py\", line 107, in fit\n",
      "    Z = np.matmul(X,self.weight)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py\", line 2016, in __array_ufunc__\n",
      "    return arraylike.array_ufunc(self, ufunc, method, *inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\pandas\\core\\arraylike.py\", line 273, in array_ufunc\n",
      "    result = maybe_dispatch_ufunc_to_dunder_op(self, ufunc, method, *inputs, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"pandas\\_libs\\ops_dispatch.pyx\", line 107, in pandas._libs.ops_dispatch.maybe_dispatch_ufunc_to_dunder_op\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py\", line 1630, in __matmul__\n",
      "    return self.dot(other)\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py\", line 1594, in dot\n",
      "    raise ValueError(\n",
      "ValueError: Dot product shape mismatch, (60, 3) vs (4, 3)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2120, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1063, in format_exception_as_a_whole\n",
      "    self.get_records(etb, number_of_lines_of_context, tb_offset) if etb else []\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1155, in get_records\n",
      "    FrameInfo(\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 780, in __init__\n",
      "    ix = inspect.getsourcelines(frame)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\Lib\\inspect.py\", line 1244, in getsourcelines\n",
      "    lines, lnum = findsource(object)\n",
      "                  ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\Lib\\inspect.py\", line 1073, in findsource\n",
      "    raise OSError('source code not available')\n",
      "OSError: source code not available\n"
     ]
    }
   ],
   "source": [
    "arguments = {'Epoch':6000, 'LearningRate':0.005}\n",
    "SoftmaxRegression = Multinomial_Regression(X,y,arguments)\n",
    "SoftmaxRegression.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5691641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6666666666666666\n",
      "Intercept: [0.40656766]\n",
      "Coefficients: [[-0.26645862 -0.00132187]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Add a small constant to X to avoid log(0) or log of negative values\n",
    "small_constant = 1e-10\n",
    "X_transformed = X + small_constant\n",
    "\n",
    "# Apply log transformation to the data to follow a lognormal distribution\n",
    "log_transformer = FunctionTransformer(np.log, validate=True)\n",
    "\n",
    "# Create a pipeline that applies the log transformation followed by the multinomial logistic regression model\n",
    "model = make_pipeline(\n",
    "    log_transformer,\n",
    "    LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    ")\n",
    "\n",
    "# Fit the model to the data\n",
    "model.fit(X_transformed, y)\n",
    "\n",
    "# Predict the class labels for the same data\n",
    "y_pred = model.predict(X_transformed)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Extract the logistic regression model from the pipeline\n",
    "logistic_model = model.named_steps['logisticregression']\n",
    "\n",
    "print(\"Intercept:\", logistic_model.intercept_)\n",
    "print(\"Coefficients:\", logistic_model.coef_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "50070956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: nan (nan)\n",
      "Intercepts: [-0.24907489  0.24907505]\n",
      "Coefficients: [[ 0.02642641  0.01748719]\n",
      " [-0.02643834 -0.01748854]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "class MultinomialLogisticRegression(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, max_iter=1000000):\n",
    "        self.max_iter = max_iter\n",
    "        self.coef_ = None\n",
    "        self.intercept_ = None\n",
    "        self.classes_ = None\n",
    "\n",
    "    def softmax(self, Z):\n",
    "        exp_Z = np.exp(Z - np.max(Z, axis=1, keepdims=True))\n",
    "        return exp_Z / exp_Z.sum(axis=1, keepdims=True)\n",
    "\n",
    "    def log_likelihood(self, params, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        n_classes = len(np.unique(y))\n",
    "        intercept = params[:n_classes]\n",
    "        coef = params[n_classes:].reshape(n_classes, n_features)\n",
    "        logits = X.dot(coef.T) + intercept\n",
    "        probs = self.softmax(logits)\n",
    "        log_likelihood = -np.sum(np.log(probs[np.arange(n_samples), y]))\n",
    "        return log_likelihood\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "        n_samples, n_features = X.shape\n",
    "        self.classes_ = np.unique(y)\n",
    "        n_classes = len(self.classes_)\n",
    "\n",
    "        # Initial parameter values\n",
    "        init_params = np.zeros(n_classes + n_classes * n_features)\n",
    "\n",
    "        # Optimize the log-likelihood function\n",
    "        result = minimize(self.log_likelihood, init_params, args=(X, y), method='L-BFGS-B', options={'maxiter': self.max_iter})\n",
    "        \n",
    "        # Extract the optimal parameters\n",
    "        intercept = result.x[:n_classes]\n",
    "        coef = result.x[n_classes:].reshape(n_classes, n_features)\n",
    "        \n",
    "        self.intercept_ = intercept\n",
    "        self.coef_ = coef\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        logits = X.dot(self.coef_.T) + self.intercept_\n",
    "        return self.softmax(logits)\n",
    "\n",
    "    def predict(self, X):\n",
    "        probs = self.predict_proba(X)\n",
    "        return np.argmax(probs, axis=1)\n",
    "\n",
    "# Example usage\n",
    "# Assuming X and y are your features and target variables\n",
    "# Encode the target variable if it's not already numeric\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# Define the model\n",
    "model = MultinomialLogisticRegression(max_iter=1000000)\n",
    "\n",
    "# Define the model evaluation procedure\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "# Evaluate the model and collect the scores\n",
    "n_scores = cross_val_score(model, X, y_encoded, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "\n",
    "# Report the model performance\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (np.mean(n_scores), np.std(n_scores)))\n",
    "\n",
    "# Fit the model to the entire dataset\n",
    "model.fit(X, y_encoded)\n",
    "print(\"Intercepts:\", model.intercept_)\n",
    "print(\"Coefficients:\", model.coef_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0c822fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  features      coef\n",
      "0     Time  0.026426\n",
      "1     Cost  0.017487\n"
     ]
    }
   ],
   "source": [
    "summary = pd.DataFrame(zip(X.columns, np.transpose(model.coef_.tolist()[0])), \n",
    "                       columns=['features', 'coef'])\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e49a950c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6666666666666666\n",
      "[0.44547239]\n",
      "[[-0.02438887 -0.02628912 -0.02624454]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "\n",
    "# Create a multinomial logistic regression model\n",
    "model = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "\n",
    "# Fit the model to the data\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predict the class labels for the same data\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(model.intercept_)\n",
    "print(model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "37bf1d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rho-squared: 0.8862496692559979\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# Sample data\n",
    "\n",
    "\n",
    "# Create a multinomial logistic regression model\n",
    "model = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='l2', \n",
    "                           C=1.0, max_iter=1000000)\n",
    "\n",
    "# Fit the model to the data\n",
    "model.fit(X, y)\n",
    "\n",
    "# Get the log-likelihood of the fitted model\n",
    "y_prob = model.predict_proba(X)\n",
    "LL_model = -log_loss(y, y_prob, normalize=False)\n",
    "\n",
    "# Fit the null model (only an intercept)\n",
    "null_model = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='l2', \n",
    "                                C=1.0, max_iter=1000000, fit_intercept=True)\n",
    "X_null = np.ones((X.shape[0], 1))  # Only intercept\n",
    "null_model.fit(X_null, y)\n",
    "\n",
    "# Get the log-likelihood of the null model\n",
    "y_null_prob = null_model.predict_proba(X_null)\n",
    "LL_null = -log_loss(y, y_null_prob, normalize=False)\n",
    "\n",
    "# Compute rho-squared\n",
    "rho_squared = 1 - (LL_model / LL_null)\n",
    "print(\"Rho-squared:\", rho_squared)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2f9d1cb7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "y_true and y_pred contain different number of classes 2, 3. Please provide the true labels explicitly through the labels argument. Classes found in y_true: [1 3]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[76], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m y_test_pred \u001b[38;5;241m=\u001b[39m model_poly\u001b[38;5;241m.\u001b[39mpredict_proba(X_test)\n\u001b[0;32m     19\u001b[0m LL_model_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mlog_loss(y_train, y_train_pred, normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 20\u001b[0m LL_model_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mlog_loss(y_test, y_test_pred, normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Fit the null model\u001b[39;00m\n\u001b[0;32m     23\u001b[0m null_model_poly \u001b[38;5;241m=\u001b[39m LogisticRegression(multi_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmultinomial\u001b[39m\u001b[38;5;124m'\u001b[39m, solver\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlbfgs\u001b[39m\u001b[38;5;124m'\u001b[39m, penalty\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml2\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m     24\u001b[0m                                      C\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000000\u001b[39m, fit_intercept\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    207\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    209\u001b[0m         )\n\u001b[0;32m    210\u001b[0m     ):\n\u001b[1;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    221\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:2897\u001b[0m, in \u001b[0;36mlog_loss\u001b[1;34m(y_true, y_pred, eps, normalize, sample_weight, labels)\u001b[0m\n\u001b[0;32m   2895\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lb\u001b[38;5;241m.\u001b[39mclasses_) \u001b[38;5;241m!=\u001b[39m y_pred\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m   2896\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 2897\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2898\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true and y_pred contain different number of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2899\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclasses \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m. Please provide the true \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2900\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels explicitly through the labels argument. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2901\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClasses found in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2902\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true: \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2903\u001b[0m                 transformed_labels\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], y_pred\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], lb\u001b[38;5;241m.\u001b[39mclasses_\n\u001b[0;32m   2904\u001b[0m             )\n\u001b[0;32m   2905\u001b[0m         )\n\u001b[0;32m   2906\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2907\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2908\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe number of classes in labels is different \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2909\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom that in y_pred. Classes found in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2910\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(lb\u001b[38;5;241m.\u001b[39mclasses_)\n\u001b[0;32m   2911\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: y_true and y_pred contain different number of classes 2, 3. Please provide the true labels explicitly through the labels argument. Classes found in y_true: [1 3]"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Example: Adding polynomial features\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the model with polynomial features\n",
    "model_poly = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='l2', \n",
    "                                C=1.0, max_iter=1000000)\n",
    "model_poly.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_train_pred = model_poly.predict_proba(X_train)\n",
    "y_test_pred = model_poly.predict_proba(X_test)\n",
    "LL_model_train = -log_loss(y_train, y_train_pred, normalize=False)\n",
    "LL_model_test = -log_loss(y_test, y_test_pred, normalize=False)\n",
    "\n",
    "# Fit the null model\n",
    "null_model_poly = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='l2', \n",
    "                                     C=1.0, max_iter=1000000, fit_intercept=True)\n",
    "null_model_poly.fit(np.ones((X_train.shape[0], 1)), y_train)\n",
    "\n",
    "# Get the log-likelihood of the null model\n",
    "y_null_train_pred = null_model_poly.predict_proba(np.ones((X_train.shape[0], 1)))\n",
    "LL_null_train = -log_loss(y_train, y_null_train_pred, normalize=False)\n",
    "\n",
    "# Compute rho-squared for training set\n",
    "rho_squared_train = 1 - (LL_model_train / LL_null_train)\n",
    "print(\"Rho-squared (train):\", rho_squared_train)\n",
    "\n",
    "# Evaluate on the test set\n",
    "y_null_test_pred = null_model_poly.predict_proba(np.ones((X_test.shape[0], 1)))\n",
    "LL_null_test = -log_loss(y_test, y_null_test_pred, normalize=False)\n",
    "\n",
    "# Compute rho-squared for test set\n",
    "rho_squared_test = 1 - (LL_model_test / LL_null_test)\n",
    "print(\"Rho-squared (test):\", rho_squared_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60f34a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.622 (0.166)\n"
     ]
    }
   ],
   "source": [
    "# define the multinomial logistic regression model with a default penalty\n",
    "model = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='l2', \n",
    "                           C=1.0, max_iter = 1000000)\n",
    "# define the model evaluation procedure\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate the model and collect the scores\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# report the model performance\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21ec1842",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e63f304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Probabilities: [0.63170614 0.36829386]\n"
     ]
    }
   ],
   "source": [
    "row = X.iloc[0:1, :]\n",
    "# predict a multinomial probability distribution\n",
    "yhat = model.predict_proba(row)\n",
    "# summarize the predicted probabilities\n",
    "print('Predicted Probabilities: %s' % yhat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45b5bbc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: 0\n"
     ]
    }
   ],
   "source": [
    "# predict the class label\n",
    "yhat = model.predict(row)\n",
    "# summarize the predicted class\n",
    "print('Predicted Class: %d' % yhat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0607802b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16d283b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2483642]\n",
      "[[-0.02640554 -0.01743611]]\n"
     ]
    }
   ],
   "source": [
    "print(result.intercept_)\n",
    "print(result.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "405144eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = pd.DataFrame(zip(X.columns, np.transpose(result.coef_.tolist()[0])), \n",
    "                       columns=['features', 'coef'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f91d55f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  features      coef\n",
      "0     Time -0.026406\n",
      "1     Cost -0.017436\n"
     ]
    }
   ],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e0111ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.600966\n",
      "         Iterations 5\n",
      "                          MNLogit Regression Results                          \n",
      "==============================================================================\n",
      "Dep. Variable:                 Choice   No. Observations:                   30\n",
      "Model:                        MNLogit   Df Residuals:                       28\n",
      "Method:                           MLE   Df Model:                            1\n",
      "Date:                Fri, 07 Jun 2024   Pseudo R-squ.:                 0.05585\n",
      "Time:                        16:13:15   Log-Likelihood:                -18.029\n",
      "converged:                       True   LL-Null:                       -19.095\n",
      "Covariance Type:            nonrobust   LLR p-value:                    0.1442\n",
      "==============================================================================\n",
      "  Choice=1       coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Time          -0.0347      0.022     -1.546      0.122      -0.079       0.009\n",
      "Cost          -0.0190      0.064     -0.298      0.766      -0.144       0.106\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example dataset\n",
    "\n",
    "\n",
    "\n",
    "# Fitting the MNLogit model\n",
    "model = sm.MNLogit(y, X)\n",
    "result = model.fit()\n",
    "\n",
    "# Display the summary of the model\n",
    "print(result.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "68bf7d66",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MNLogit' object has no attribute 'predict_proba'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m row \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m1\u001b[39m, :]\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# predict a multinomial probability distribution\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m yhat \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict_proba(row)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# summarize the predicted probabilities\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPredicted Probabilities: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m yhat[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'MNLogit' object has no attribute 'predict_proba'"
     ]
    }
   ],
   "source": [
    "row = X.iloc[0:1, :]\n",
    "# predict a multinomial probability distribution\n",
    "yhat = model.predict_proba(row)\n",
    "# summarize the predicted probabilities\n",
    "print('Predicted Probabilities: %s' % yhat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bf2f1764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.596139\n",
      "         Iterations 6\n",
      "                          MNLogit Regression Results                          \n",
      "==============================================================================\n",
      "Dep. Variable:                 Choice   No. Observations:                   30\n",
      "Model:                        MNLogit   Df Residuals:                       27\n",
      "Method:                           MLE   Df Model:                            2\n",
      "Date:                Thu, 06 Jun 2024   Pseudo R-squ.:                 0.06343\n",
      "Time:                        22:20:42   Log-Likelihood:                -17.884\n",
      "converged:                       True   LL-Null:                       -19.095\n",
      "Covariance Type:            nonrobust   LLR p-value:                    0.2978\n",
      "==============================================================================\n",
      "  Choice=1       coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.4982      0.954      0.522      0.601      -1.371       2.367\n",
      "Time          -0.0529      0.044     -1.188      0.235      -0.140       0.034\n",
      "Cost          -0.0350      0.076     -0.459      0.646      -0.184       0.114\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "# Fitting the MNLogit model without an intercept\n",
    "model = sm.MNLogit(y, X)\n",
    "result = model.fit()\n",
    "\n",
    "# Display the summary of the model\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "079d3d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.24836398]\n",
      "[[-0.02640551 -0.01743616]]\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(random_state=0, multi_class='multinomial', solver='newton-cg')\n",
    "result = model.fit(X, y)\n",
    "print(result.intercept_)\n",
    "print(result.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "17421e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.596139\n",
      "         Iterations 6\n",
      "                          MNLogit Regression Results                          \n",
      "==============================================================================\n",
      "Dep. Variable:                 Choice   No. Observations:                   30\n",
      "Model:                        MNLogit   Df Residuals:                       27\n",
      "Method:                           MLE   Df Model:                            2\n",
      "Date:                Thu, 06 Jun 2024   Pseudo R-squ.:                 0.06343\n",
      "Time:                        22:33:38   Log-Likelihood:                -17.884\n",
      "converged:                       True   LL-Null:                       -19.095\n",
      "Covariance Type:            nonrobust   LLR p-value:                    0.2978\n",
      "==============================================================================\n",
      "  Choice=1       coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.4982      0.954      0.522      0.601      -1.371       2.367\n",
      "Time          -0.0529      0.044     -1.188      0.235      -0.140       0.034\n",
      "Cost          -0.0350      0.076     -0.459      0.646      -0.184       0.114\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "# Fitting the MNLogit model without an intercept using Newton-Raphson method\n",
    "X = sm.add_constant(X)\n",
    "model = sm.MNLogit(y, X)\n",
    "result = model.fit(method='newton')\n",
    "\n",
    "# Display the summary of the model\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3b94ed90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xlogit\n",
      "  Obtaining dependency information for xlogit from https://files.pythonhosted.org/packages/5b/ab/5280d6920d59e739063effb59b54349a71ee66bdd11daa1664d08e68564a/xlogit-0.2.7-py3-none-any.whl.metadata\n",
      "  Downloading xlogit-0.2.7-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy>=1.13.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from xlogit) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from xlogit) (1.11.1)\n",
      "Downloading xlogit-0.2.7-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: xlogit\n",
      "Successfully installed xlogit-0.2.7\n"
     ]
    }
   ],
   "source": [
    "!pip install xlogit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e1aaedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1= []\n",
    "p2 = []\n",
    "\n",
    "# Initialize an empty list to store predicted probabilities\n",
    "for i in range(len(hsb2.Mode)):\n",
    "    row = X.iloc[i:i+1, :]  # Select a single row from the DataFrame\n",
    "    # Predict a multinomial probability distribution\n",
    "    yhat = model.predict_proba(row)\n",
    "    # Summarize the predicted probabilities and append to the list\n",
    "    p1.append(yhat[0][0])\n",
    "    p2.append(yhat[0][1])\n",
    "  \n",
    "hsb2['P1'] = p1\n",
    "hsb2['P2'] = p2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f8526bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: 0\n"
     ]
    }
   ],
   "source": [
    "# predict the class label\n",
    "yhat = model.predict(row)\n",
    "# summarize the predicted class\n",
    "print('Predicted Class: %d' % yhat[0])\n",
    "\n",
    "c = []\n",
    "# Initialize an empty list to store predicted probabilities\n",
    "for i in range(len(hsb2.Mode)):\n",
    "    row = X.iloc[i:i+1, :]  # Select a single row from the DataFrame\n",
    "    # Predict a multinomial probability distribution\n",
    "    yhat = model.predict(row)\n",
    "    # Summarize the predicted probabilities and append to the list\n",
    "    c.append(yhat[0])\n",
    "    \n",
    "\n",
    "hsb2['Predicted'] = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bbadcc19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Person No.  Mode  Time  Cost  Choice        P1        P2  Predicted\n",
      "0            1     1    15     7       1  0.631706  0.368294          0\n",
      "1            1     2    30     0       0  0.747930  0.252070          0\n",
      "2            1     3    20     3       0  0.660183  0.339817          0\n",
      "3            2     1    10     7       0  0.568440  0.431560          0\n",
      "4            2     2    15     0       1  0.573327  0.426673          0\n",
      "5            2     3    10     3       0  0.533947  0.466053          0\n",
      "6            3     1     5     7       0  0.502858  0.497142          0\n",
      "7            3     2    20     0       0  0.636335  0.363665          0\n",
      "8            3     3     7     3       1  0.494392  0.505608          1\n",
      "9            4     1    25    25       0  0.844927  0.155073          0\n",
      "10           4     2    60     0       0  0.935350  0.064650          0\n",
      "11           4     3    20     5       1  0.675650  0.324350          0\n",
      "12           5     1    10     7       0  0.568440  0.431560          0\n",
      "13           5     2    12     0       1  0.534198  0.465802          0\n",
      "14           5     3    10     3       0  0.533947  0.466053          0\n",
      "15           6     1    20     7       1  0.690744  0.309256          0\n",
      "16           6     2    35     0       0  0.794401  0.205599          0\n",
      "17           6     3    23     5       0  0.709362  0.290638          0\n",
      "18           7     1    25    27       0  0.853847  0.146153          0\n",
      "19           7     2    90     0       0  0.986023  0.013977          0\n",
      "20           7     3    20     7       1  0.690744  0.309256          0\n",
      "21           8     1    15     7       1  0.631706  0.368294          0\n",
      "22           8     2    30     0       0  0.747930  0.252070          0\n",
      "23           8     3    20     3       0  0.660183  0.339817          0\n",
      "24           9     1    10     7       0  0.568440  0.431560          0\n",
      "25           9     2    15     0       1  0.573327  0.426673          0\n",
      "26           9     3    10     3       0  0.533947  0.466053          0\n",
      "27          10     1    25     8       1  0.750733  0.249267          0\n",
      "28          10     2    20     0       0  0.636335  0.363665          0\n",
      "29          10     3    25     5       0  0.730647  0.269353          0\n"
     ]
    }
   ],
   "source": [
    "print(hsb2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5e9ec7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
