{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "03f4bce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>SubVehID</th>\n",
       "      <th>Choice</th>\n",
       "      <th>Gap</th>\n",
       "      <th>RelativeSpeed</th>\n",
       "      <th>TW</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  SubVehID  Choice  Gap  RelativeSpeed  TW\n",
       "0   1         9       1   12              0   0\n",
       "1   2        13       0   16              2   1\n",
       "2   3        13       2   15              1   1\n",
       "3   4        13       0   15              0   1\n",
       "4   5        36       1   12              0   0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import requests\n",
    "import io\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from matplotlib import pyplot\n",
    "from statsmodels.discrete.discrete_model import MNLogit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "import math\n",
    "from scipy import stats\n",
    "hsb2 = pd.read_csv(\"C:\\\\Users\\\\Admin\\\\Desktop\\\\Vaishnavi\\\\Auto\\\\DriverData1.csv\")\n",
    "\n",
    "hsb2[\"Gap\"] = hsb2[\"Gap\"].astype('int64')\n",
    "hsb2[\"RelativeSpeed\"] = hsb2[\"RelativeSpeed\"].astype('int64')\n",
    "hsb2[\"TW\"] = hsb2[\"TW\"].astype('int64')\n",
    "hsb2[\"Choice\"] = hsb2[\"Choice\"].astype('int64')\n",
    "hsb2['Choice']=hsb2['Choice']-1\n",
    "\n",
    "hsb3 = hsb2\n",
    "hsb3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6598f057",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = hsb3['Choice']\n",
    "X = hsb3.drop(['ID','SubVehID','Choice'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dae1849a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gap</th>\n",
       "      <th>RelativeSpeed</th>\n",
       "      <th>TW</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Gap  RelativeSpeed  TW\n",
       "0   12              0   0\n",
       "1   16              2   1\n",
       "2   15              1   1\n",
       "3   15              0   1\n",
       "4   12              0   0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ea0134bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2400"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7d2ab4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "799d8bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept and Coefficients:\n",
      "[[ 0.          0.          0.          0.        ]\n",
      " [-2.60573396  0.10927262  0.12924574 -0.10253815]\n",
      " [-3.83474207  0.27906785  0.49454685 -0.37028896]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Define the softmax function\n",
    "def softmax(z):\n",
    "    z = np.array(z)  # Ensure z is a numpy array\n",
    "    z = z - np.max(z, axis=1, keepdims=True)  # For numerical stability\n",
    "    exp_z = np.exp(z)\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "# Define the log-likelihood function\n",
    "def log_likelihood(params, X, y, num_classes):\n",
    "    num_samples, num_features = X.shape\n",
    "    # Add zero coefficients for the reference class\n",
    "    params = params.reshape((num_classes - 1, num_features + 1))\n",
    "    beta = np.vstack([np.zeros((1, num_features + 1)), params])\n",
    "    X_intercept = np.hstack([np.ones((num_samples, 1)), X])  # Add intercept term\n",
    "    logits = np.dot(X_intercept, beta.T)\n",
    "    probabilities = softmax(logits)\n",
    "    ll = np.sum(np.log(probabilities[np.arange(num_samples), y]))\n",
    "    return -ll  # We negate because we will be minimizing\n",
    "\n",
    "# Fit the model\n",
    "def fit_multinomial_logistic_regression(X, y, num_classes, max_iter=100):\n",
    "    num_samples, num_features = X.shape\n",
    "    initial_params = np.zeros((num_classes - 1, num_features + 1)).ravel()\n",
    "    result = minimize(log_likelihood, initial_params, args=(X, y, num_classes),\n",
    "                      method='L-BFGS-B', options={'maxiter': max_iter})\n",
    "    # Add zero coefficients for the reference class to the result\n",
    "    beta = np.vstack([np.zeros((1, num_features + 1)), result.x.reshape(num_classes - 1, num_features + 1)])\n",
    "    return beta\n",
    "\n",
    "# Example usage with some sample data\n",
    "# Replace this with your actual data\n",
    "\n",
    "num_classes = 3  # Define the number of classes based on your dataset\n",
    "\n",
    "# Fit the model\n",
    "beta = fit_multinomial_logistic_regression(X, y, num_classes)\n",
    "\n",
    "print(\"Intercept and Coefficients:\")\n",
    "print(beta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "85a60d71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 2084.308097\n",
      "         Iterations: 1238\n",
      "         Function evaluations: 1283\n",
      "         Gradient evaluations: 1283\n",
      "         Hessian evaluations: 1238\n",
      "Intercept and Coefficients:\n",
      "[[ 0.92607206 -0.04095365 -0.25565245  0.26005539]\n",
      " [-0.11744499 -0.03362288 -0.0715817   0.05380435]\n",
      " [-0.77485275  0.10156383  0.30553459 -0.28856603]]\n",
      "Log-likelihood: -2084.3080972934204\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "# Define the softmax function\n",
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # numerical stability\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "# Define the log-likelihood function\n",
    "def log_likelihood(params, X, y, num_classes):\n",
    "    num_samples, num_features = X.shape\n",
    "    # Reshape the parameter array into a coefficient matrix\n",
    "    beta = params.reshape(num_classes, num_features + 1)\n",
    "    # Add a column of ones for the intercept term\n",
    "    X_intercept = np.hstack([np.ones((num_samples, 1)), X])\n",
    "    \n",
    "    # Compute the linear combination\n",
    "    logits = np.dot(X_intercept, beta.T)\n",
    "    # Compute the probabilities using the softmax function\n",
    "    probabilities = softmax(logits)\n",
    "    # Compute the log-likelihood\n",
    "    ll = np.sum(np.log(probabilities[np.arange(num_samples), y]))\n",
    "    return -ll  # Return the negative log-likelihood for minimization\n",
    "\n",
    "# Define the gradient of the log-likelihood function\n",
    "def gradient(params, X, y, num_classes):\n",
    "    num_samples, num_features = X.shape\n",
    "    params = params.reshape(num_classes, num_features + 1)\n",
    "    X = np.hstack([np.ones((num_samples, 1)), X])  # Add intercept term\n",
    "    logits = np.dot(X, params.T)\n",
    "    probs = np.exp(logits - logsumexp(logits, axis=1, keepdims=True))\n",
    "    indicator = np.zeros_like(probs)\n",
    "    indicator[np.arange(num_samples), y] = 1\n",
    "    grad = np.dot((indicator - probs).T, X)\n",
    "    return -grad.ravel()\n",
    "\n",
    "# Define the Hessian of the log-likelihood function\n",
    "def hessian(params, X, y, num_classes):\n",
    "    num_samples, num_features = X.shape\n",
    "    params = params.reshape(num_classes, num_features + 1)\n",
    "    X = np.hstack([np.ones((num_samples, 1)), X])  # Add intercept term\n",
    "    logits = np.dot(X, params.T)\n",
    "    probs = np.exp(logits - logsumexp(logits, axis=1, keepdims=True))\n",
    "    hess = np.zeros((num_classes, num_features + 1, num_classes, num_features + 1))\n",
    "    for i in range(num_samples):\n",
    "        xi = X[i].reshape(-1, 1)\n",
    "        pi = probs[i].reshape(-1, 1)\n",
    "        hess += np.einsum('ij,kl->ikjl', pi @ (1 - pi).T, xi @ xi.T)\n",
    "    return hess.reshape((num_classes * (num_features + 1), num_classes * (num_features + 1)))\n",
    "\n",
    "# Define a function to fit the model using Newton's method\n",
    "def fit_multinomial_logistic_regression(X, y, num_classes, max_iter=1000000):\n",
    "    num_samples, num_features = X.shape\n",
    "    # Initialize parameters with small random values\n",
    "    initial_params = np.random.randn(num_classes * (num_features + 1)) * 0.01\n",
    "    # Optimize the log-likelihood function using Newton's method\n",
    "    result = minimize(log_likelihood, initial_params, args=(X, y, num_classes),\n",
    "                      method='Newton-CG', jac=gradient, hess=hessian, options={'maxiter': max_iter, 'disp': True})\n",
    "    # Reshape the result into the coefficient matrix\n",
    "    beta = result.x.reshape(num_classes, num_features + 1)\n",
    "    return beta\n",
    "\n",
    "# Example data\n",
    "# Define your input feature matrix X and target vector y\n",
    "\n",
    "num_classes = 3\n",
    "\n",
    "# Fit the model\n",
    "beta = fit_multinomial_logistic_regression(X, y, num_classes)\n",
    "\n",
    "# Print the coefficients\n",
    "print(\"Intercept and Coefficients:\")\n",
    "print(beta)\n",
    "\n",
    "# Compute and print the log-likelihood of the model\n",
    "ll_model = -log_likelihood(beta.ravel(), X, y, num_classes)\n",
    "print(\"Log-likelihood:\", ll_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f1730b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept and Coefficients:\n",
      "[[ 2.14666262 -0.12960308 -0.20794964  0.15761092]\n",
      " [-0.45839896 -0.0203722  -0.07868068  0.05502989]\n",
      " [-1.68828305  0.14947839  0.28660041 -0.21266022]]\n",
      "-2072.8760776524578\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Define the softmax function\n",
    "def softmax(z):\n",
    "    return np.exp(z) / np.sum(np.exp(z), axis=1, keepdims=True)\n",
    "\n",
    "# Define the log-likelihood function\n",
    "def log_likelihood(params, X, y, num_classes):\n",
    "    num_samples, num_features = X.shape\n",
    "    # Reshape the parameter array into a coefficient matrix\n",
    "    beta = params.reshape(num_classes, num_features + 1)\n",
    "    # Add a column of ones for the intercept term\n",
    "    X_intercept = np.hstack([np.ones((num_samples, 1)), X])\n",
    "    \n",
    "    # Compute the linear combination\n",
    "    logits = np.dot(X_intercept, beta.T)\n",
    "    # Compute the probabilities using the softmax function\n",
    "    probabilities = softmax(logits)\n",
    "    # Compute the log-likelihood\n",
    "    ll = np.sum(np.log(probabilities[np.arange(num_samples), y]))\n",
    "    return -ll  # Return the negative log-likelihood for minimization\n",
    "\n",
    "# Define a function to fit the model\n",
    "def fit_multinomial_logistic_regression(X, y, num_classes, max_iter=1000000):\n",
    "    num_samples, num_features = X.shape\n",
    "    # Initialize parameters\n",
    "    initial_params = np.zeros((num_classes, num_features + 1)).ravel()\n",
    "    # Optimize the log-likelihood function\n",
    "    result = minimize(log_likelihood, initial_params, args=(X, y, num_classes),\n",
    "                      method='L-BFGS-B', options={'maxiter': max_iter})\n",
    "    # Reshape the result into the coefficient matrix\n",
    "    beta = result.x.reshape(num_classes, num_features + 1)\n",
    "    return beta\n",
    "\n",
    "\n",
    "num_classes = 3\n",
    "\n",
    "\n",
    "\n",
    "# Fit the model\n",
    "beta = fit_multinomial_logistic_regression(X, y, num_classes)\n",
    "\n",
    "\n",
    "print(\"Intercept and Coefficients:\")\n",
    "print(beta)\n",
    "\n",
    "ll_model=-log_likelihood(beta, X, y, num_classes)\n",
    "print(ll_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "ecfa9d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood_P(params, X, y, num_classes):\n",
    "    num_samples, num_features = X.shape\n",
    "    # Reshape the parameter array into a coefficient matrix\n",
    "    beta = params.reshape(num_classes, num_features + 1)\n",
    "    # Add a column of ones for the intercept term\n",
    "    X_intercept = np.hstack([np.ones((num_samples, 1)), X])\n",
    "    # Compute the linear combination\n",
    "    logits = np.dot(X_intercept, beta.T)\n",
    "    # Compute the probabilities using the softmax function\n",
    "    probabilities = softmax(logits)\n",
    "    # Compute the log-likelihood\n",
    "    return probabilities  # Return the negative log-l\n",
    "p=log_likelihood_P(beta, X, y, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "ef20b29c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[191], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m p\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "p.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "7d60d016",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_p = np.amax(p, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7c71e81e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1. 12.  0.  0.]\n",
      " [ 1. 16.  2.  1.]\n",
      " [ 1. 15.  1.  1.]\n",
      " ...\n",
      " [ 1. 13.  0.  1.]\n",
      " [ 1. 15.  3.  1.]\n",
      " [ 1. 15.  4.  1.]]\n"
     ]
    }
   ],
   "source": [
    "X_intercept = np.hstack([np.ones((2400, 1)), X])\n",
    "print(X_intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "4e579c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.63615009 -0.35692377 -0.62291368 ... -0.60835857 -0.29870018\n",
      " -0.20059712]\n"
     ]
    }
   ],
   "source": [
    "log_p=np.log(max_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "ca8b77df",
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_1=np.sum(log_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "264eec71",
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_0=X.shape[0]*math.log(1/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8da1f1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5285.581778398234\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hsb4 = pd.read_csv(\"C:\\\\Users\\\\Admin\\\\Desktop\\\\Vaishnavi\\\\Auto\\\\Book2.csv\")\n",
    "\n",
    "hsb4[\"Gap\"] = hsb4[\"Gap\"].astype('int64')\n",
    "hsb4[\"RelativeSpeed\"] = hsb4[\"RelativeSpeed\"].astype('int64')\n",
    "hsb4[\"TW\"] = hsb4[\"TW\"].astype('int64')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Assuming beta, y, num_classes are defined somewhere in your code\n",
    "ll_00 = -log_likelihood(beta, hsb4, y, num_classes)\n",
    "print(ll_00)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "976a89cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ll_model: -2072.8760776524578\n",
      "ll_null: -2636.6694928034635\n",
      "ll_00 -5285.581778398234\n"
     ]
    }
   ],
   "source": [
    "print(\"ll_model:\",ll_model)\n",
    "print(\"ll_null:\",ll_0)\n",
    "print(\"ll_00\",ll_00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "58bcd2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       1\n",
      "1       0\n",
      "2       2\n",
      "3       0\n",
      "4       1\n",
      "       ..\n",
      "2395    0\n",
      "2396    2\n",
      "2397    0\n",
      "2398    2\n",
      "2399    1\n",
      "Name: Choice, Length: 2400, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713a4748",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8e5d5202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Rho-squared: 0.2138278675768146\n",
      "Rho-squared0: 0.6078244241487015\n",
      "Chi-square statistic: 1127.5868303020115\n",
      "p-value: 7.93451693153249e-243\n"
     ]
    }
   ],
   "source": [
    "chi_square_stat = 2 * (ll_model - ll_0)\n",
    "df_model = X.shape[1] * (len(np.unique(y)) - 1) \n",
    "df_null = len(np.unique(y)) - 1  # Number of intercepts in the null model\n",
    "df_diff = df_model - df_null\n",
    "p_value = stats.chi2.sf(chi_square_stat, df_diff)\n",
    "rho_squared = 1 - (ll_model / ll_0)\n",
    "rho_squared_00 = 1 - (ll_model / ll_00)\n",
    "print(\" \")\n",
    "print(\"Rho-squared:\", rho_squared)\n",
    "print(\"Rho-squared0:\", rho_squared_00)\n",
    "print(\"Chi-square statistic:\", chi_square_stat)\n",
    "print(\"p-value:\", p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "4cb7ffa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Indices of maximum value in each row:\n",
      "[0 2 2 ... 0 2 2]\n"
     ]
    }
   ],
   "source": [
    "# Get the index of the maximum value in each row\n",
    "max_indices = np.argmax(p, axis=1)\n",
    "\n",
    "print(\"\\nIndices of maximum value in each row:\")\n",
    "print(max_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "e1a65108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2], dtype=int64)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(max_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbdeeec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d216d698",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def softmax(beta, X):\n",
    "    \"\"\"\n",
    "    Compute softmax probabilities for each alternative.\n",
    "    \n",
    "    Parameters:\n",
    "    beta : array_like\n",
    "        Coefficient vector to be estimated.\n",
    "    X : array_like\n",
    "        Matrix of explanatory variables (including intercept).\n",
    "        \n",
    "    Returns:\n",
    "    probabilities : array_like\n",
    "        Softmax probabilities for each alternative.\n",
    "    \"\"\"\n",
    "    # Reshape beta to ensure it's a 1D array\n",
    "    beta = np.reshape(beta, (X.shape[1],))\n",
    "    \n",
    "    exp_utilities = np.exp(np.dot(X, beta))\n",
    "    sum_exp_utilities = np.sum(exp_utilities, axis=1, keepdims=True)\n",
    "    probabilities = exp_utilities / sum_exp_utilities\n",
    "    return probabilities\n",
    "\n",
    "def log_likelihood(beta, X, y):\n",
    "    \"\"\"\n",
    "    Compute the log-likelihood function for multinomial logit model.\n",
    "    \n",
    "    Parameters:\n",
    "    beta : array_like\n",
    "        Coefficient vector to be estimated.\n",
    "    X : array_like\n",
    "        Matrix of explanatory variables (including intercept).\n",
    "    y : array_like\n",
    "        Vector of choices made by individuals.\n",
    "        \n",
    "    Returns:\n",
    "    log_likelihood_value : float\n",
    "        Log-likelihood value.\n",
    "    \"\"\"\n",
    "    probabilities = softmax(beta, X)\n",
    "    log_likelihood_value = np.sum(np.log(probabilities[np.arange(len(X)), y]))\n",
    "    return log_likelihood_value\n",
    "\n",
    "# Example data (predefined X and y)\n",
    "# X should have dimensions (N, K), where N is the number of observations and K is the number of explanatory variables (including intercept)\n",
    "# y should be a vector of length N containing integers representing the chosen alternative (starting from 0)\n",
    "\n",
    "\n",
    "# Initial guess for beta (can be zeros or any initial guess)\n",
    "initial_beta = np.zeros(X.shape[1])\n",
    "\n",
    "# Define the negative log-likelihood function (to be minimized)\n",
    "negative_log_likelihood = lambda beta: -log_likelihood(beta, X, y)\n",
    "\n",
    "# Minimize the negative log-likelihood function to find the estimated beta values\n",
    "result = minimize(negative_log_likelihood, initial_beta, method='BFGS')\n",
    "\n",
    "# Estimated beta values\n",
    "estimated_beta = result.x\n",
    "\n",
    "# Maximum log-likelihood value (negative of the minimized value)\n",
    "max_log_likelihood_value = -result.fun\n",
    "\n",
    "print(\"Estimated beta values:\", estimated_beta)\n",
    "print(\"Maximum log-likelihood value:\", max_log_likelihood_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1dd619",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a2743d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc83092",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "951532b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Betas (including intercept):\n",
      "[[ 2.16795085 -0.13141009 -0.21033779  0.16740135]\n",
      " [-0.43761449 -0.02214737 -0.08109084  0.06485632]\n",
      " [-1.66692163  0.14766567  0.28419696 -0.20285649]]\n",
      "Maximum Log-Likelihood Value: -2072.876077306807\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Example data (features X and choices y are assumed to be defined)\n",
    "# Assuming X has shape (N, D) and y has shape (N,)\n",
    "N, D = X.shape\n",
    "J = len(np.unique(y))  # Number of unique classes in y\n",
    "\n",
    "# Add intercept term to X\n",
    "X_intercept = np.hstack((np.ones((N, 1)), X))\n",
    "\n",
    "# Define the likelihood function for multinomial logit model with intercept\n",
    "def multinomial_logit_likelihood_with_intercept(betas, X_intercept, y):\n",
    "    J = len(betas) // (D + 1)  # Number of alternatives\n",
    "    N = X_intercept.shape[0]  # Number of observations\n",
    "    betas = betas.reshape((J, D + 1))  # Reshape betas to (J, D + 1)\n",
    "    likelihood = 0.0\n",
    "    \n",
    "    # Calculate the log-likelihood\n",
    "    for i in range(N):\n",
    "        x_i = X_intercept[i]\n",
    "        exp_terms = np.exp(np.dot(betas, x_i))\n",
    "        probs = exp_terms / np.sum(exp_terms)\n",
    "        likelihood += np.log(probs[y[i]])\n",
    "    \n",
    "    return -likelihood  # Minimize negative likelihood\n",
    "\n",
    "# Initial guess for betas (including intercept)\n",
    "initial_betas = np.zeros(J * (D + 1))\n",
    "\n",
    "# Find the maximum likelihood estimates (MLE) using optimization (e.g., scipy's minimize)\n",
    "result = minimize(multinomial_logit_likelihood_with_intercept, initial_betas, args=(X_intercept, y), method='BFGS')\n",
    "estimated_betas = result.x.reshape((J, D + 1))  # Reshape estimated betas\n",
    "\n",
    "# Calculate maximum log-likelihood value\n",
    "max_log_likelihood = -result.fun  # result.fun gives the minimum negative log-likelihood\n",
    "\n",
    "print(\"Estimated Betas (including intercept):\")\n",
    "print(estimated_betas)\n",
    "print(f\"Maximum Log-Likelihood Value: {max_log_likelihood}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "45aa4eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Log-Likelihood Value: -2072.876077306807\n"
     ]
    }
   ],
   "source": [
    "print(f\"Maximum Log-Likelihood Value: {max_log_likelihood}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e81f15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "3658f1bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept and Coefficients:\n",
      "[[ 0.00000000e+00 -7.82449067e-07 -7.82449067e-07  0.00000000e+00]\n",
      " [ 0.00000000e+00 -7.82449067e-07 -7.82449067e-07  0.00000000e+00]\n",
      " [ 0.00000000e+00 -7.82449067e-07 -7.82449067e-07  0.00000000e+00]]\n",
      "-8348.354781588952\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Define the softmax function\n",
    "def softmax(z):\n",
    "    return np.exp(z) / np.sum(np.exp(z), axis=1, keepdims=True)\n",
    "\n",
    "# Define the log-likelihood function\n",
    "def log_likelihood(params, X, y, frequencies, num_classes):\n",
    "    num_samples, num_features = X.shape\n",
    "    # Reshape the parameter array into a coefficient matrix\n",
    "    beta = params.reshape(num_classes, num_features + 1)\n",
    "    # Add a column of ones for the intercept term\n",
    "    X_intercept = np.hstack([np.ones((num_samples, 1)), X])\n",
    "    # Compute the linear combination\n",
    "    logits = np.dot(X_intercept, beta.T)\n",
    "    # Compute the probabilities using the softmax function\n",
    "    probabilities = softmax(logits)\n",
    "    \n",
    "    # Weight the probabilities by frequencies\n",
    "    weighted_probabilities = probabilities * frequencies[:, np.newaxis]\n",
    "    \n",
    "    # Compute the log-likelihood\n",
    "    ll = np.sum(np.log(weighted_probabilities[np.arange(num_samples), y]))\n",
    "    return -ll  # Return the negative log-likelihood for minimization\n",
    "\n",
    "# Define a function to fit the model with frequency information\n",
    "def fit_multinomial_logistic_regression(X, y, frequencies, num_classes, max_iter=1000000):\n",
    "    num_samples, num_features = X.shape\n",
    "    # Initialize parameters\n",
    "    initial_params = np.zeros((num_classes, num_features + 1)).ravel()\n",
    "    # Optimize the log-likelihood function\n",
    "    result = minimize(log_likelihood, initial_params, args=(X, y, frequencies, num_classes),\n",
    "                      method='L-BFGS-B', options={'maxiter': max_iter})\n",
    "    # Reshape the result into the coefficient matrix\n",
    "    beta = result.x.reshape(num_classes, num_features + 1)\n",
    "    return beta\n",
    "\n",
    "# Example usage:\n",
    "# Suppose X is your feature matrix, y is your target vector (labels), and frequencies is a vector/array of frequencies.\n",
    "\n",
    "# Replace these with your actual data\n",
    "    # Your target vector (labels)\n",
    "frequencies = hsb3.['Subvehicle']     # Frequency array corresponding to each sample\n",
    "\n",
    "num_classes = len(np.unique(y))   # Number of unique classes in y\n",
    "\n",
    "# Fit the multinomial logistic regression model\n",
    "beta = fit_multinomial_logistic_regression(X, y, frequencies, num_classes)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Intercept and Coefficients:\")\n",
    "print(beta)\n",
    "\n",
    "ll_1=-log_likelihood(beta, X, y, num_classes)\n",
    "print(ll_1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "3f1f8495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400\n",
      "-2636.6694928034635\n"
     ]
    }
   ],
   "source": [
    "N=y.count()\n",
    "print(N)\n",
    "ll_0=N*math.log(1/3)\n",
    "print(ll_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "6f39e829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Rho-squared: -2.166249999999995\n",
      "Chi-square statistic: -11423.370577570979\n",
      "p-value: 1.0\n"
     ]
    }
   ],
   "source": [
    "chi_square_stat = 2 * (ll_1 - ll_0)\n",
    "df_model = X.shape[1] * (len(np.unique(y)) - 1) \n",
    "df_null = len(np.unique(y)) - 1  # Number of intercepts in the null model\n",
    "df_diff = df_model - df_null\n",
    "p_value = stats.chi2.sf(chi_square_stat, df_diff)\n",
    "rho_squared = 1 - (ll_1 / ll_0)\n",
    "print(\" \")\n",
    "print(\"Rho-squared:\", rho_squared)\n",
    "print(\"Chi-square statistic:\", chi_square_stat)\n",
    "print(\"p-value:\", p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874725a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b103bfff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c580982",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882d1698",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda46e85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d1e0a4b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400\n"
     ]
    }
   ],
   "source": [
    "N=y.count()\n",
    "print(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "622ce851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2636.6694928034635\n"
     ]
    }
   ],
   "source": [
    "ll_0=N*math.log(1/3)\n",
    "print(ll_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2cc49896",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[90], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m feature_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgap\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspd\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtw\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Print coefficients with feature names\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m print_coefficients(beta, feature_names)\n",
      "Cell \u001b[1;32mIn[90], line 3\u001b[0m, in \u001b[0;36mprint_coefficients\u001b[1;34m(beta, feature_names)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprint_coefficients\u001b[39m(beta, feature_names):\n\u001b[1;32m----> 3\u001b[0m     num_classes, num_features \u001b[38;5;241m=\u001b[39m beta\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_classes):\n\u001b[0;32m      5\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCoefficients for class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Function to print coefficients with feature names\n",
    "def print_coefficients(beta, feature_names):\n",
    "    num_classes, num_features = beta.shape\n",
    "    for i in range(num_classes):\n",
    "        print(f\"Coefficients for class {i}:\")\n",
    "        for j in range(1, num_features):  # Start from 1 to skip the intercept\n",
    "            print(f\"{feature_names[j-1]}: {beta[i, j]}\")\n",
    "        print(f\"Intercept: {beta[i, 0]}\\n\")\n",
    "\n",
    "# Example usage\n",
    "num_classes = 3\n",
    "beta = fit_multinomial_logistic_regression(X, y, num_classes)\n",
    "\n",
    "# List of feature names (assuming you have them in order)\n",
    "feature_names = ['gap', 'spd',\"tw\"]\n",
    "\n",
    "# Print coefficients with feature names\n",
    "print_coefficients(beta, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31262e25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6e41cfe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients for class 'dec':\n",
      "gap: -7.824490665322885e-07\n",
      "spd: -7.824490665322885e-07\n",
      "tw: 0.0\n",
      "Intercept: 0.0\n",
      "\n",
      "Coefficients for class 'cons':\n",
      "gap: -7.824490665322885e-07\n",
      "spd: -7.824490665322885e-07\n",
      "tw: 0.0\n",
      "Intercept: 0.0\n",
      "\n",
      "Coefficients for class 'acce':\n",
      "gap: -7.824490665322885e-07\n",
      "spd: -7.824490665322885e-07\n",
      "tw: 0.0\n",
      "Intercept: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from scipy.special import softmax\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Define the softmax function\n",
    "def softmax(z):\n",
    "    return np.exp(z) / np.sum(np.exp(z), axis=1, keepdims=True)\n",
    "\n",
    "# Define the log-likelihood function for multinomial logistic regression\n",
    "def log_likelihood(params, X, y, num_classes):\n",
    "    num_samples, num_features = X.shape\n",
    "    # Reshape the parameter array into a coefficient matrix\n",
    "    beta = params.reshape(num_classes, num_features + 1)\n",
    "    # Add a column of ones for the intercept term\n",
    "    X_intercept = np.hstack([np.ones((num_samples, 1)), X])\n",
    "    \n",
    "    # Compute the linear combination\n",
    "    logits = np.dot(X_intercept, beta.T)\n",
    "    # Compute the probabilities using the softmax function\n",
    "    probabilities = softmax(logits)\n",
    "    \n",
    "    # Compute the log-likelihood\n",
    "    ll = 0.0\n",
    "    for i in range(num_samples):\n",
    "        # For each sample, accumulate log-likelihood contribution for the chosen classes\n",
    "        ll += np.sum(y[i] * np.log(probabilities[i]))\n",
    "    \n",
    "    return -ll  # Return the negative log-likelihood for minimization\n",
    "\n",
    "# Define a function to fit the model\n",
    "def fit_multinomial_logistic_regression(X, y, num_classes, max_iter=1000000):\n",
    "    num_samples, num_features = X.shape\n",
    "    # Initialize parameters\n",
    "    initial_params = np.zeros((num_classes, num_features + 1)).ravel()\n",
    "    # Optimize the log-likelihood function\n",
    "    result = minimize(log_likelihood, initial_params, args=(X, y, num_classes),\n",
    "                      method='L-BFGS-B', options={'maxiter': max_iter})\n",
    "    # Reshape the result into the coefficient matrix\n",
    "    beta = result.x.reshape(num_classes, num_features + 1)\n",
    "    return beta\n",
    "\n",
    "# Function to print coefficients with feature names and class names\n",
    "def print_coefficients(beta, feature_names, class_names):\n",
    "    num_classes, num_features = beta.shape\n",
    "    for i in range(num_classes):\n",
    "        print(f\"Coefficients for class '{class_names[i]}':\")\n",
    "        for j in range(1, num_features):  # Start from 1 to skip the intercept\n",
    "            print(f\"{feature_names[j-1]}: {beta[i, j]}\")\n",
    "        print(f\"Intercept: {beta[i, 0]}\\n\")\n",
    "\n",
    "# Example usage\n",
    "num_classes = 3\n",
    "class_names = ['dec', 'cons', 'acce']  # Class names corresponding to y values\n",
    "\n",
    "# Example usage\n",
    "\n",
    "beta = fit_multinomial_logistic_regression(X, y, num_classes)\n",
    "\n",
    "# List of feature names (assuming you have them in order)\n",
    "feature_names = ['gap', 'spd','tw']\n",
    "\n",
    "# Print coefficients with feature names\n",
    "print_coefficients(beta, feature_names,class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "73e6db7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Rho-squared: 0.2138278675768146\n",
      "Chi-square statistic: 1127.5868303020115\n",
      "p-value: 7.93451693153249e-243\n"
     ]
    }
   ],
   "source": [
    "chi_square_stat = 2 * (ll_1 - ll_0)\n",
    "df_model = X.shape[1] * (len(np.unique(y)) - 1) \n",
    "df_null = len(np.unique(y)) - 1  # Number of intercepts in the null model\n",
    "df_diff = df_model - df_null\n",
    "p_value = stats.chi2.sf(chi_square_stat, df_diff)\n",
    "rho_squared = 1 - (ll_1 / ll_0)\n",
    "print(\" \")\n",
    "print(\"Rho-squared:\", rho_squared)\n",
    "print(\"Chi-square statistic:\", chi_square_stat)\n",
    "print(\"p-value:\", p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "73783355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients for class 'dec':\n",
      "gap: 0.0\n",
      "spd: 0.0\n",
      "tw: 0.0\n",
      "Intercept: 0.0\n",
      "\n",
      "Coefficients for class 'cons':\n",
      "gap: 0.10932086784170261\n",
      "spd: 0.12922810103162571\n",
      "tw: -0.10257731228108642\n",
      "Intercept: -2.606421385401063\n",
      "\n",
      "Coefficients for class 'acce':\n",
      "gap: 0.27913042131451715\n",
      "spd: 0.49452602443656085\n",
      "tw: -0.370206676585683\n",
      "Intercept: -3.8357109346585503\n",
      "\n",
      "Intercept and Coefficients:\n",
      "[[ 0.          0.          0.          0.        ]\n",
      " [-2.60642139  0.10932087  0.1292281  -0.10257731]\n",
      " [-3.83571093  0.27913042  0.49452602 -0.37020668]]\n",
      "Log-likelihood: -2072.876079841275\n",
      "Chi-squared: 5740.246314558834\n",
      "p-value: 0.0\n",
      "Rho-squared (McFadden's R-squared): 1.7861721332533302\n",
      "t-statistics: [[ 0.          0.          0.          0.        ]\n",
      " [-0.07498555  0.04920799  0.07088888 -0.02011733]\n",
      " [-0.17518421  0.19886795  3.67872317 -0.08309977]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from scipy.special import softmax\n",
    "from scipy.stats import chi2\n",
    "\n",
    "# Define the softmax function\n",
    "def softmax(z):\n",
    "    return np.exp(z) / np.sum(np.exp(z), axis=1, keepdims=True)\n",
    "\n",
    "# Define the log-likelihood function\n",
    "def log_likelihood(params, X, y, num_classes):\n",
    "    num_samples, num_features = X.shape\n",
    "    # Reshape the parameter array into a coefficient matrix\n",
    "    beta = params.reshape(num_classes, num_features + 1)\n",
    "    # Ensure the first row of beta remains zero\n",
    "    beta[0, :] = 0\n",
    "    # Add a column of ones for the intercept term\n",
    "    X_intercept = np.hstack([np.ones((num_samples, 1)), X])\n",
    "    \n",
    "    # Compute the linear combination\n",
    "    logits = np.dot(X_intercept, beta.T)\n",
    "    # Compute the probabilities using the softmax function\n",
    "    probabilities = softmax(logits)\n",
    "    # Compute the log-likelihood\n",
    "    ll = np.sum(np.log(probabilities[np.arange(num_samples), y]))\n",
    "    return -ll  # Return the negative log-likelihood for minimization\n",
    "\n",
    "# Define a function to fit the model\n",
    "def fit_multinomial_logistic_regression(X, y, num_classes, max_iter=1000000):\n",
    "    num_samples, num_features = X.shape\n",
    "    # Initialize parameters, with the first row set to zero\n",
    "    initial_params = np.zeros((num_classes, num_features + 1))\n",
    "    initial_params = initial_params.ravel()\n",
    "    # Optimize the log-likelihood function\n",
    "    result = minimize(log_likelihood, initial_params, args=(X, y, num_classes),\n",
    "                      method='L-BFGS-B', options={'maxiter': max_iter})\n",
    "    # Reshape the result into the coefficient matrix\n",
    "    beta = result.x.reshape(num_classes, num_features + 1)\n",
    "    # Ensure the first row of beta remains zero\n",
    "    beta[0, :] = 0\n",
    "    return beta, result\n",
    "\n",
    "\n",
    "# Function to print coefficients with feature names and class names\n",
    "def print_coefficients(beta, feature_names, class_names):\n",
    "    num_classes, num_features = beta.shape\n",
    "    for i in range(num_classes):\n",
    "        print(f\"Coefficients for class '{class_names[i]}':\")\n",
    "        for j in range(1, num_features):  # Start from 1 to skip the intercept\n",
    "            print(f\"{feature_names[j-1]}: {beta[i, j]}\")\n",
    "        print(f\"Intercept: {beta[i, 0]}\\n\")\n",
    "\n",
    "# Example usage\n",
    "num_classes = 3\n",
    "class_names = ['dec', 'cons', 'acce']  # Class names corresponding to y values\n",
    "\n",
    "# Example usage\n",
    "\n",
    "beta,result = fit_multinomial_logistic_regression(X, y, num_classes)\n",
    "\n",
    "# List of feature names (assuming you have them in order)\n",
    "feature_names = ['gap', 'spd','tw']\n",
    "\n",
    "# Print coefficients with feature names\n",
    "print_coefficients(beta, feature_names,class_names)\n",
    "\n",
    "# Goodness-of-fit tests and metrics\n",
    "def calculate_goodness_of_fit(X, y, beta, result, num_classes):\n",
    "    num_samples, num_features = X.shape\n",
    "    # Add a column of ones for the intercept term\n",
    "    X_intercept = np.hstack([np.ones((num_samples, 1)), X])\n",
    "    \n",
    "    # Compute the linear combination\n",
    "    logits = np.dot(X_intercept, beta.T)\n",
    "    # Compute the probabilities using the softmax function\n",
    "    probabilities = softmax(logits)\n",
    "    \n",
    "    # Chi-squared test\n",
    "    observed = np.zeros((num_samples, num_classes))\n",
    "    observed[np.arange(num_samples), y] = 1\n",
    "    expected = probabilities\n",
    "    chi_squared = np.sum((observed - expected) ** 2 / expected)\n",
    "    p_value = chi2.sf(chi_squared, df=(num_classes - 1) * (num_features + 1))\n",
    "    \n",
    "    # Rho-squared (McFadden's R-squared)\n",
    "    null_model_ll = -num_samples * np.log(1 / num_classes)\n",
    "    fitted_model_ll = -result.fun\n",
    "    rho_squared = 1 - (fitted_model_ll / null_model_ll)\n",
    "    \n",
    "    # t-statistics (Wald test)\n",
    "    hessian_inv = result.hess_inv.todense() if hasattr(result.hess_inv, \"todense\") else result.hess_inv\n",
    "    std_errors = np.sqrt(np.diag(hessian_inv).reshape(num_classes, num_features + 1))\n",
    "    t_stats = beta / std_errors\n",
    "    \n",
    "    return chi_squared, p_value, rho_squared, t_stats\n",
    "\n",
    "# Example usage:\n",
    "# X and y need to be defined with your data\n",
    "# num_classes = 3\n",
    "\n",
    "# Fit the model\n",
    "beta, result = fit_multinomial_logistic_regression(X, y, num_classes)\n",
    "\n",
    "# Calculate goodness-of-fit metrics\n",
    "chi_squared, p_value, rho_squared, t_stats = calculate_goodness_of_fit(X, y, beta, result, num_classes)\n",
    "\n",
    "print(\"Intercept and Coefficients:\")\n",
    "print(beta)\n",
    "print(\"Log-likelihood:\", -log_likelihood(beta.ravel(), X, y, num_classes))\n",
    "print(\"Chi-squared:\", chi_squared)\n",
    "print(\"p-value:\", p_value)\n",
    "print(\"Rho-squared (McFadden's R-squared):\", rho_squared)\n",
    "print(\"t-statistics:\", t_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be796b60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3dfc76aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (2400,4) and (5,3) not aligned: 4 (dim 1) != 5 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 45\u001b[0m\n\u001b[0;32m     43\u001b[0m X[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX_intercept\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     44\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m---> 45\u001b[0m beta \u001b[38;5;241m=\u001b[39m fit_multinomial_logistic_regression(X, y, num_classes)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIntercept and Coefficients:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(beta)\n",
      "Cell \u001b[1;32mIn[4], line 29\u001b[0m, in \u001b[0;36mfit_multinomial_logistic_regression\u001b[1;34m(X, y, num_classes)\u001b[0m\n\u001b[0;32m     27\u001b[0m     initial_params \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((num_classes, num_features \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mravel()\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;66;03m# Optimize the log-likelihood function\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m     result \u001b[38;5;241m=\u001b[39m minimize(log_likelihood, initial_params, args\u001b[38;5;241m=\u001b[39m(X, y, num_classes),\n\u001b[0;32m     30\u001b[0m                       method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mL-BFGS-B\u001b[39m\u001b[38;5;124m'\u001b[39m, options \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaxiter\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1000000\u001b[39m,  \u001b[38;5;66;03m# maximum number of iterations\u001b[39;00m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisp\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m,    \u001b[38;5;66;03m# display convergence messages\u001b[39;00m\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgtol\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1e-6\u001b[39m     \u001b[38;5;66;03m# tolerance for termination\u001b[39;00m\n\u001b[0;32m     34\u001b[0m })\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;66;03m# Reshape the result into the coefficient matrix\u001b[39;00m\n\u001b[0;32m     36\u001b[0m     beta \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mx\u001b[38;5;241m.\u001b[39mreshape(num_classes, num_features \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_minimize.py:710\u001b[0m, in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    707\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[0;32m    708\u001b[0m                              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml-bfgs-b\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 710\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0;32m    711\u001b[0m                            callback\u001b[38;5;241m=\u001b[39mcallback, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m    712\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtnc\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    713\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[0;32m    714\u001b[0m                         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_lbfgsb_py.py:307\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[1;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    305\u001b[0m         iprint \u001b[38;5;241m=\u001b[39m disp\n\u001b[1;32m--> 307\u001b[0m sf \u001b[38;5;241m=\u001b[39m _prepare_scalar_function(fun, x0, jac\u001b[38;5;241m=\u001b[39mjac, args\u001b[38;5;241m=\u001b[39margs, epsilon\u001b[38;5;241m=\u001b[39meps,\n\u001b[0;32m    308\u001b[0m                               bounds\u001b[38;5;241m=\u001b[39mnew_bounds,\n\u001b[0;32m    309\u001b[0m                               finite_diff_rel_step\u001b[38;5;241m=\u001b[39mfinite_diff_rel_step)\n\u001b[0;32m    311\u001b[0m func_and_grad \u001b[38;5;241m=\u001b[39m sf\u001b[38;5;241m.\u001b[39mfun_and_grad\n\u001b[0;32m    313\u001b[0m fortran_int \u001b[38;5;241m=\u001b[39m _lbfgsb\u001b[38;5;241m.\u001b[39mtypes\u001b[38;5;241m.\u001b[39mintvar\u001b[38;5;241m.\u001b[39mdtype\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_optimize.py:383\u001b[0m, in \u001b[0;36m_prepare_scalar_function\u001b[1;34m(fun, x0, jac, args, bounds, epsilon, finite_diff_rel_step, hess)\u001b[0m\n\u001b[0;32m    379\u001b[0m     bounds \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf, np\u001b[38;5;241m.\u001b[39minf)\n\u001b[0;32m    381\u001b[0m \u001b[38;5;66;03m# ScalarFunction caches. Reuse of fun(x) during grad\u001b[39;00m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;66;03m# calculation reduces overall function evaluations.\u001b[39;00m\n\u001b[1;32m--> 383\u001b[0m sf \u001b[38;5;241m=\u001b[39m ScalarFunction(fun, x0, args, grad, hess,\n\u001b[0;32m    384\u001b[0m                     finite_diff_rel_step, bounds, epsilon\u001b[38;5;241m=\u001b[39mepsilon)\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sf\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:158\u001b[0m, in \u001b[0;36mScalarFunction.__init__\u001b[1;34m(self, fun, x0, args, grad, hess, finite_diff_rel_step, finite_diff_bounds, epsilon)\u001b[0m\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m fun_wrapped(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx)\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun_impl \u001b[38;5;241m=\u001b[39m update_fun\n\u001b[1;32m--> 158\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun()\n\u001b[0;32m    160\u001b[0m \u001b[38;5;66;03m# Gradient evaluation\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(grad):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:251\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated:\n\u001b[1;32m--> 251\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun_impl()\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:155\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[1;34m()\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_fun\u001b[39m():\n\u001b[1;32m--> 155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m fun_wrapped(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:137\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnfev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m fx \u001b[38;5;241m=\u001b[39m fun(np\u001b[38;5;241m.\u001b[39mcopy(x), \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    138\u001b[0m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
      "Cell \u001b[1;32mIn[4], line 16\u001b[0m, in \u001b[0;36mlog_likelihood\u001b[1;34m(params, X, y, num_classes)\u001b[0m\n\u001b[0;32m     13\u001b[0m beta\u001b[38;5;241m=\u001b[39m params\u001b[38;5;241m.\u001b[39mreshape(num_classes, num_features \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Add a column of ones for the intercept term    \u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Compute the linear combination\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m logits \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(X, beta\u001b[38;5;241m.\u001b[39mT)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Compute the probabilities using the softmax function\u001b[39;00m\n\u001b[0;32m     18\u001b[0m probabilities \u001b[38;5;241m=\u001b[39m softmax(logits)\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (2400,4) and (5,3) not aligned: 4 (dim 1) != 5 (dim 0)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from scipy.special import softmax\n",
    "\n",
    "# Define the softmax function\n",
    "def softmax(z):\n",
    "    return np.exp(z) / np.sum(np.exp(z), axis=1, keepdims=True)\n",
    "\n",
    "# Define the log-likelihood function\n",
    "def log_likelihood(params, X, y, num_classes):\n",
    "    num_samples, num_features = X.shape    \n",
    "    # Reshape the parameter array into a coefficient matrix\n",
    "    beta= params.reshape(num_classes, num_features + 1)\n",
    "    # Add a column of ones for the intercept term    \n",
    "    # Compute the linear combination\n",
    "    logits = np.dot(X, beta.T)\n",
    "    # Compute the probabilities using the softmax function\n",
    "    probabilities = softmax(logits)\n",
    "    # Compute the log-likelihood\n",
    "    ll = np.sum(np.log(probabilities[np.arange(num_samples), y]))\n",
    "    return -ll  # Return the negative log-likelihood for minimization\n",
    "\n",
    "# Define a function to fit the model\n",
    "def fit_multinomial_logistic_regression(X, y, num_classes):\n",
    "    num_samples, num_features = X.shape\n",
    "    # Initialize parameters\n",
    "    initial_params = np.zeros((num_classes, num_features + 1)).ravel()\n",
    "    # Optimize the log-likelihood function\n",
    "    result = minimize(log_likelihood, initial_params, args=(X, y, num_classes),\n",
    "                      method='L-BFGS-B', options = {\n",
    "    'maxiter': 1000000,  # maximum number of iterations\n",
    "    'disp': True,    # display convergence messages\n",
    "    'gtol': 1e-6     # tolerance for termination\n",
    "})\n",
    "    # Reshape the result into the coefficient matrix\n",
    "    beta = result.x.reshape(num_classes, num_features + 1)\n",
    "    return beta\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Fit the model\n",
    "X['X_intercept'] = 1\n",
    "num_classes = 3\n",
    "beta = fit_multinomial_logistic_regression(X, y, num_classes)\n",
    "\n",
    "print(\"Intercept and Coefficients:\")\n",
    "print(beta)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae846b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept and Coefficients:\n",
      "[[ 1.0733351  -0.12952322 -0.20793152  0.15763137  1.0733351 ]\n",
      " [-0.22929011 -0.02027999 -0.07867769  0.05505686 -0.22928899]\n",
      " [-0.84403822  0.1495465   0.28661315 -0.21268005 -0.84403855]]\n",
      "-2072.876077423387\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from scipy.special import softmax\n",
    "\n",
    "# Define the softmax function\n",
    "def softmax(z):\n",
    "    return np.exp(z) / np.sum(np.exp(z), axis=1, keepdims=True)\n",
    "\n",
    "# Define the log-likelihood function\n",
    "def log_likelihood(params, X, y, num_classes):\n",
    "    num_samples, num_features = X.shape\n",
    "    # Reshape the parameter array into a coefficient matrix\n",
    "    beta = params.reshape(num_classes, num_features + 1)\n",
    "    # Add a column of ones for the intercept term\n",
    "    X_intercept = np.hstack([np.ones((num_samples, 1)), X])\n",
    "    \n",
    "    # Compute the linear combination\n",
    "    logits = np.dot(X_intercept, beta.T)\n",
    "    # Compute the probabilities using the softmax function\n",
    "    probabilities = softmax(logits)\n",
    "    # Compute the log-likelihood\n",
    "    ll = np.sum(np.log(probabilities[np.arange(num_samples), y]))\n",
    "    return -ll  # Return the negative log-likelihood for minimization\n",
    "\n",
    "# Define a function to fit the model\n",
    "def fit_multinomial_logistic_regression(X, y, num_classes, max_iter=1000000):\n",
    "    num_samples, num_features = X.shape\n",
    "    # Initialize parameters\n",
    "    initial_params = np.zeros((num_classes, num_features + 1)).ravel()\n",
    "    # Optimize the log-likelihood function\n",
    "    result = minimize(log_likelihood, initial_params, args=(X, y, num_classes),\n",
    "                      method='L-BFGS-B', options={'maxiter': max_iter})\n",
    "    # Reshape the result into the coefficient matrix\n",
    "    beta = result.x.reshape(num_classes, num_features + 1)\n",
    "    return beta\n",
    "\n",
    "\n",
    "num_classes = 3\n",
    "\n",
    "\n",
    "\n",
    "# Fit the model\n",
    "beta = fit_multinomial_logistic_regression(X, y, num_classes)\n",
    "\n",
    "\n",
    "print(\"Intercept and Coefficients:\")\n",
    "print(beta)\n",
    "\n",
    "ll_1=-log_likelihood(beta, X, y, num_classes)\n",
    "print(ll_1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a39a7b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.14474441 -0.12946674 -0.20794989  0.15763029]\n",
      " [-0.45772992 -0.02042404 -0.07874445  0.0553777 ]\n",
      " [-1.68702506  0.14941304  0.28668445 -0.2130071 ]]\n",
      "Null Log Likelihood: -2361.8493280848334\n",
      "-2072.8761203526383\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Define the softmax function\n",
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # for numerical stability\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "# Define the log-likelihood function\n",
    "def log_likelihood(params, X, y, num_classes):\n",
    "    num_samples, num_features = X.shape\n",
    "    # Reshape the parameter array into a coefficient matrix\n",
    "    beta = params.reshape(num_classes, num_features + 1)\n",
    "    # Add a column of ones for the intercept term\n",
    "    X_intercept = np.hstack([np.ones((num_samples, 1)), X])\n",
    "    \n",
    "    # Compute the linear combination\n",
    "    logits = np.dot(X_intercept, beta.T)\n",
    "    # Compute the probabilities using the softmax function\n",
    "    probabilities = softmax(logits)\n",
    "    # Compute the log-likelihood\n",
    "    ll = np.sum(np.log(probabilities[np.arange(num_samples), y]))\n",
    "    return -ll  # Return the negative log-likelihood for minimization\n",
    "\n",
    "# Define a function to fit the model\n",
    "def fit_multinomial_logistic_regression(X, y, num_classes, max_iter=1000000):\n",
    "    num_samples, num_features = X.shape\n",
    "    # Initialize parameters\n",
    "    initial_params = np.zeros((num_classes, num_features + 1)).ravel()\n",
    "    # Optimize the log-likelihood function\n",
    "    result = minimize(log_likelihood, initial_params, args=(X, y, num_classes),\n",
    "                      method='L-BFGS-B', options={'maxiter': max_iter})\n",
    "    # Reshape the result into the coefficient matrix\n",
    "    beta = result.x.reshape(num_classes, num_features + 1)\n",
    "    return beta\n",
    "\n",
    "# Define the function to calculate the null log likelihood\n",
    "def null_log_likelihood(y, num_classes):\n",
    "    num_samples = len(y)\n",
    "    # Calculate the baseline probabilities\n",
    "    class_counts = np.bincount(y, minlength=num_classes)\n",
    "    class_probs = class_counts / num_samples\n",
    "    # Compute the null log likelihood\n",
    "    null_ll = np.sum(class_counts * np.log(class_probs))\n",
    "    return null_ll\n",
    "\n",
    "# Example usage\n",
    "num_classes = 3\n",
    "\n",
    "    # Fit the model\n",
    "beta = fit_multinomial_logistic_regression(X, y, num_classes)\n",
    "print(beta)\n",
    "    # Calculate the null log likelihood\n",
    "null_ll = null_log_likelihood(y, num_classes)\n",
    "print(\"Null Log Likelihood:\", null_ll)\n",
    "print(-log_likelihood(beta, X, y, num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2954ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chi-Square Statistic: 577.9464154643902\n",
      "P-Value: 1.2866735176769592e-119\n",
      "McFadden's R-Squared: 0.12235039902673062\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import chi2\n",
    "\n",
    "# Define the softmax function\n",
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # for numerical stability\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "# Define the log-likelihood function\n",
    "def log_likelihood(params, X, y, num_classes):\n",
    "    num_samples, num_features = X.shape\n",
    "    # Reshape the parameter array into a coefficient matrix\n",
    "    beta = params.reshape(num_classes, num_features + 1)\n",
    "    # Add a column of ones for the intercept term\n",
    "    X_intercept = np.hstack([np.ones((num_samples, 1)), X])\n",
    "    \n",
    "    # Compute the linear combination\n",
    "    logits = np.dot(X_intercept, beta.T)\n",
    "    # Compute the probabilities using the softmax function\n",
    "    probabilities = softmax(logits)\n",
    "    # Compute the log-likelihood\n",
    "    ll = np.sum(np.log(probabilities[np.arange(num_samples), y]))\n",
    "    return -ll  # Return the negative log-likelihood for minimization\n",
    "\n",
    "# Define a function to fit the model\n",
    "def fit_multinomial_logistic_regression(X, y, num_classes, max_iter=1000000):\n",
    "    num_samples, num_features = X.shape\n",
    "    # Initialize parameters\n",
    "    initial_params = np.zeros((num_classes, num_features + 1)).ravel()\n",
    "    # Optimize the log-likelihood function\n",
    "    result = minimize(log_likelihood, initial_params, args=(X, y, num_classes),\n",
    "                      method='L-BFGS-B', options={'maxiter': max_iter})\n",
    "    # Reshape the result into the coefficient matrix\n",
    "    beta = result.x.reshape(num_classes, num_features + 1)\n",
    "    return beta, -result.fun  # Return the beta coefficients and log likelihood\n",
    "\n",
    "# Define the function to calculate the null log likelihood\n",
    "def null_log_likelihood(y, num_classes):\n",
    "    num_samples = len(y)\n",
    "    # Calculate the baseline probabilities\n",
    "    class_counts = np.bincount(y, minlength=num_classes)\n",
    "    class_probs = class_counts / num_samples\n",
    "    # Compute the null log likelihood\n",
    "    null_ll = np.sum(class_counts * np.log(class_probs))\n",
    "    return null_ll\n",
    "\n",
    "# Chi-square test, p-value, and rho-squared\n",
    "def model_comparison(X, y, num_classes):\n",
    "    # Fit the model\n",
    "    beta, ll_full = fit_multinomial_logistic_regression(X, y, num_classes)\n",
    "    \n",
    "    # Calculate the null log likelihood\n",
    "    ll_null = null_log_likelihood(y, num_classes)\n",
    "    \n",
    "    # Chi-square statistic\n",
    "    chi_square_stat = 2 * (ll_full - ll_null)\n",
    "    \n",
    "    # Degrees of freedom\n",
    "    dof = (X.shape[1] + 1) * (num_classes - 1)\n",
    "    \n",
    "    # P-value\n",
    "    p_value = chi2.sf(chi_square_stat, dof)\n",
    "    \n",
    "    # McFadden's R-squared\n",
    "    rho_squared = 1 - (ll_full / ll_null)\n",
    "    \n",
    "    return chi_square_stat, p_value, rho_squared\n",
    "\n",
    "\n",
    "\n",
    "    # Perform model comparison\n",
    "chi_square_stat, p_value, rho_squared = model_comparison(X, y, num_classes)\n",
    "print(\"Chi-Square Statistic:\", chi_square_stat)\n",
    "print(\"P-Value:\", p_value)\n",
    "print(\"McFadden's R-Squared:\", rho_squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "803cfc99",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 12 into shape (2,4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 69\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;66;03m# Fit the model\u001b[39;00m\n\u001b[0;32m     68\u001b[0m beta \u001b[38;5;241m=\u001b[39m fit_multinomial_logistic_regression(X, y, num_classes)\n\u001b[1;32m---> 69\u001b[0m ll_1\u001b[38;5;241m=\u001b[39mlog_likelihood(beta, X, y, num_classes)\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;66;03m# Calculate the null log likelihood\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mll_1:\u001b[39m\u001b[38;5;124m\"\u001b[39m,ll_1)\n",
      "Cell \u001b[1;32mIn[12], line 14\u001b[0m, in \u001b[0;36mlog_likelihood\u001b[1;34m(params, X, y, num_classes)\u001b[0m\n\u001b[0;32m     12\u001b[0m num_samples, num_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Insert a row of zeros for the reference class\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m beta \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack([params\u001b[38;5;241m.\u001b[39mreshape(num_classes \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, num_features \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m, num_features \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))])\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Add a column of ones for the intercept term\u001b[39;00m\n\u001b[0;32m     16\u001b[0m X_intercept \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack([np\u001b[38;5;241m.\u001b[39mones((num_samples, \u001b[38;5;241m1\u001b[39m)), X])\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 12 into shape (2,4)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from scipy.special import softmax\n",
    "\n",
    "# Define the softmax function\n",
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # for numerical stability\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "# Define the log-likelihood function\n",
    "def log_likelihood(params, X, y, num_classes):\n",
    "    num_samples, num_features = X.shape\n",
    "    # Insert a row of zeros for the reference class\n",
    "    beta = np.vstack([params.reshape(num_classes - 1, num_features + 1), np.zeros((1, num_features + 1))])\n",
    "    # Add a column of ones for the intercept term\n",
    "    X_intercept = np.hstack([np.ones((num_samples, 1)), X])\n",
    "    \n",
    "    # Compute the linear combination\n",
    "    logits = np.dot(X_intercept, beta.T)\n",
    "    # Compute the probabilities using the softmax function\n",
    "    probabilities = softmax(logits)\n",
    "    # Compute the log-likelihood\n",
    "    ll = np.sum(np.log(probabilities[np.arange(num_samples), y]))\n",
    "    return -ll  # Return the negative log-likelihood for minimization\n",
    "\n",
    "# Define a function to fit the model\n",
    "def fit_multinomial_logistic_regression(X, y, num_classes, max_iter=1000000):\n",
    "    num_samples, num_features = X.shape\n",
    "    # Initialize parameters for (num_classes - 1) * (num_features + 1)\n",
    "    initial_params = np.zeros(((num_classes - 1) * (num_features + 1)))\n",
    "    # Optimize the log-likelihood function\n",
    "    result = minimize(log_likelihood, initial_params, args=(X, y, num_classes),\n",
    "                      method='L-BFGS-B', options={'maxiter': max_iter})\n",
    "    # Insert a row of zeros for the reference class in the final beta\n",
    "    beta = np.vstack([result.x.reshape(num_classes - 1, num_features + 1), np.zeros((1, num_features + 1))])\n",
    "    return beta\n",
    "\n",
    "# Null log likelihood\n",
    "def null_log_likelihood(y, num_classes):\n",
    "    num_samples = len(y)\n",
    "    class_counts = np.bincount(y, minlength=num_classes)\n",
    "    class_probs = class_counts / num_samples\n",
    "    null_ll = np.sum(class_counts * np.log(class_probs))\n",
    "    return -null_ll\n",
    "\n",
    "# Chi-Square test and p-value\n",
    "def chi_square_test(ll_null, ll_fitted, df):\n",
    "    chi_square_stat = 2 * (ll_null - ll_fitted)\n",
    "    p_value = chi2.sf(chi_square_stat, df)\n",
    "    return chi_square_stat, p_value\n",
    "\n",
    "# Pseudo R-Squared\n",
    "def rho_squared(ll_null, ll_fitted):\n",
    "    return 1 - (ll_fitted / ll_null)\n",
    "\n",
    "# T-statistics\n",
    "def t_statistics(beta, X, y, num_classes):\n",
    "    num_samples, num_features = X.shape\n",
    "    X_intercept = np.hstack([np.ones((num_samples, 1)), X])\n",
    "    model = sm.MNLogit(y, X_intercept)\n",
    "    result = model.fit(method='newton', maxiter=100000, disp=False)\n",
    "    return result.tvalues\n",
    "\n",
    "\n",
    "num_classes = 3\n",
    "\n",
    "    # Fit the model\n",
    "beta = fit_multinomial_logistic_regression(X, y, num_classes)\n",
    "ll_1=log_likelihood(beta, X, y, num_classes)\n",
    "    # Calculate the null log likelihood\n",
    "print(\"ll_1:\",ll_1)\n",
    "ll_null = null_log_likelihood(y, num_classes)\n",
    "print(\"ll_null:\",ll_null)\n",
    "    # Chi-Square test\n",
    "df = (num_classes - 1) * (X.shape[1] + 1)\n",
    "ll_fitted = -log_likelihood(beta[:-1].ravel(), X, y, num_classes)\n",
    "chi_square_stat, p_value = chi_square_test(ll_null, ll_fitted, df)\n",
    "print(\"Chi-Square Statistic:\", chi_square_stat)\n",
    "print(\"P-Value:\", p_value)\n",
    "\n",
    "    # Rho-Squared\n",
    "rho2 = rho_squared(ll_null, ll_fitted) \n",
    "print(\"Rho-Squared:\", rho2)\n",
    "\n",
    "    # T-Statistics\n",
    "t_stats = t_statistics(beta, X, y, num_classes)\n",
    "print(\"T-Statistics:\\n\", t_stats)\n",
    "\n",
    "print(\"     \")\n",
    "    # Print beta coefficients\n",
    "print(\"Intercept and Coefficients:\")\n",
    "print(beta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83e3b7af",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m logit_model \u001b[38;5;241m=\u001b[39m DiscreteModel(y, X, nev_factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Fit the model\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m logit_results \u001b[38;5;241m=\u001b[39m logit_model\u001b[38;5;241m.\u001b[39mfit(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbfgs\u001b[39m\u001b[38;5;124m'\u001b[39m, maxiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, disp\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Print the summary\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(logit_results\u001b[38;5;241m.\u001b[39msummary())\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:243\u001b[0m, in \u001b[0;36mDiscreteModel.fit\u001b[1;34m(self, start_params, method, maxiter, full_output, disp, callback, **kwargs)\u001b[0m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# TODO: make a function factory to have multiple call-backs\u001b[39;00m\n\u001b[1;32m--> 243\u001b[0m mlefit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(start_params\u001b[38;5;241m=\u001b[39mstart_params,\n\u001b[0;32m    244\u001b[0m                      method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[0;32m    245\u001b[0m                      maxiter\u001b[38;5;241m=\u001b[39mmaxiter,\n\u001b[0;32m    246\u001b[0m                      full_output\u001b[38;5;241m=\u001b[39mfull_output,\n\u001b[0;32m    247\u001b[0m                      disp\u001b[38;5;241m=\u001b[39mdisp,\n\u001b[0;32m    248\u001b[0m                      callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[0;32m    249\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mlefit\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\model.py:566\u001b[0m, in \u001b[0;36mLikelihoodModel.fit\u001b[1;34m(self, start_params, method, maxiter, full_output, disp, fargs, callback, retall, skip_hessian, **kwargs)\u001b[0m\n\u001b[0;32m    563\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_t\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    565\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m Optimizer()\n\u001b[1;32m--> 566\u001b[0m xopt, retvals, optim_settings \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39m_fit(f, score, start_params,\n\u001b[0;32m    567\u001b[0m                                                fargs, kwargs,\n\u001b[0;32m    568\u001b[0m                                                hessian\u001b[38;5;241m=\u001b[39mhess,\n\u001b[0;32m    569\u001b[0m                                                method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[0;32m    570\u001b[0m                                                disp\u001b[38;5;241m=\u001b[39mdisp,\n\u001b[0;32m    571\u001b[0m                                                maxiter\u001b[38;5;241m=\u001b[39mmaxiter,\n\u001b[0;32m    572\u001b[0m                                                callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[0;32m    573\u001b[0m                                                retall\u001b[38;5;241m=\u001b[39mretall,\n\u001b[0;32m    574\u001b[0m                                                full_output\u001b[38;5;241m=\u001b[39mfull_output)\n\u001b[0;32m    575\u001b[0m \u001b[38;5;66;03m# Restore cov_type, cov_kwds and use_t\u001b[39;00m\n\u001b[0;32m    576\u001b[0m optim_settings\u001b[38;5;241m.\u001b[39mupdate(kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\optimizer.py:242\u001b[0m, in \u001b[0;36mOptimizer._fit\u001b[1;34m(self, objective, gradient, start_params, fargs, kwargs, hessian, method, maxiter, full_output, disp, callback, retall)\u001b[0m\n\u001b[0;32m    239\u001b[0m     fit_funcs\u001b[38;5;241m.\u001b[39mupdate(extra_fit_funcs)\n\u001b[0;32m    241\u001b[0m func \u001b[38;5;241m=\u001b[39m fit_funcs[method]\n\u001b[1;32m--> 242\u001b[0m xopt, retvals \u001b[38;5;241m=\u001b[39m func(objective, gradient, start_params, fargs, kwargs,\n\u001b[0;32m    243\u001b[0m                      disp\u001b[38;5;241m=\u001b[39mdisp, maxiter\u001b[38;5;241m=\u001b[39mmaxiter, callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[0;32m    244\u001b[0m                      retall\u001b[38;5;241m=\u001b[39mretall, full_output\u001b[38;5;241m=\u001b[39mfull_output,\n\u001b[0;32m    245\u001b[0m                      hess\u001b[38;5;241m=\u001b[39mhessian)\n\u001b[0;32m    247\u001b[0m optim_settings \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m'\u001b[39m: method, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart_params\u001b[39m\u001b[38;5;124m'\u001b[39m: start_params,\n\u001b[0;32m    248\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaxiter\u001b[39m\u001b[38;5;124m'\u001b[39m: maxiter, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfull_output\u001b[39m\u001b[38;5;124m'\u001b[39m: full_output,\n\u001b[0;32m    249\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisp\u001b[39m\u001b[38;5;124m'\u001b[39m: disp, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfargs\u001b[39m\u001b[38;5;124m'\u001b[39m: fargs, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcallback\u001b[39m\u001b[38;5;124m'\u001b[39m: callback,\n\u001b[0;32m    250\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mretall\u001b[39m\u001b[38;5;124m'\u001b[39m: retall, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextra_fit_funcs\u001b[39m\u001b[38;5;124m\"\u001b[39m: extra_fit_funcs}\n\u001b[0;32m    251\u001b[0m optim_settings\u001b[38;5;241m.\u001b[39mupdate(kwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\optimizer.py:537\u001b[0m, in \u001b[0;36m_fit_bfgs\u001b[1;34m(f, score, start_params, fargs, kwargs, disp, maxiter, callback, retall, full_output, hess)\u001b[0m\n\u001b[0;32m    535\u001b[0m norm \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnorm\u001b[39m\u001b[38;5;124m'\u001b[39m, np\u001b[38;5;241m.\u001b[39mInf)\n\u001b[0;32m    536\u001b[0m epsilon \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepsilon\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1.4901161193847656e-08\u001b[39m)\n\u001b[1;32m--> 537\u001b[0m retvals \u001b[38;5;241m=\u001b[39m optimize\u001b[38;5;241m.\u001b[39mfmin_bfgs(f, start_params, score, args\u001b[38;5;241m=\u001b[39mfargs,\n\u001b[0;32m    538\u001b[0m                              gtol\u001b[38;5;241m=\u001b[39mgtol, norm\u001b[38;5;241m=\u001b[39mnorm, epsilon\u001b[38;5;241m=\u001b[39mepsilon,\n\u001b[0;32m    539\u001b[0m                              maxiter\u001b[38;5;241m=\u001b[39mmaxiter, full_output\u001b[38;5;241m=\u001b[39mfull_output,\n\u001b[0;32m    540\u001b[0m                              disp\u001b[38;5;241m=\u001b[39mdisp, retall\u001b[38;5;241m=\u001b[39mretall, callback\u001b[38;5;241m=\u001b[39mcallback)\n\u001b[0;32m    541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m full_output:\n\u001b[0;32m    542\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m retall:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_optimize.py:1359\u001b[0m, in \u001b[0;36mfmin_bfgs\u001b[1;34m(f, x0, fprime, args, gtol, norm, epsilon, maxiter, full_output, disp, retall, callback, xrtol)\u001b[0m\n\u001b[0;32m   1351\u001b[0m opts \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgtol\u001b[39m\u001b[38;5;124m'\u001b[39m: gtol,\n\u001b[0;32m   1352\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnorm\u001b[39m\u001b[38;5;124m'\u001b[39m: norm,\n\u001b[0;32m   1353\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m'\u001b[39m: epsilon,\n\u001b[0;32m   1354\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisp\u001b[39m\u001b[38;5;124m'\u001b[39m: disp,\n\u001b[0;32m   1355\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaxiter\u001b[39m\u001b[38;5;124m'\u001b[39m: maxiter,\n\u001b[0;32m   1356\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreturn_all\u001b[39m\u001b[38;5;124m'\u001b[39m: retall}\n\u001b[0;32m   1358\u001b[0m callback \u001b[38;5;241m=\u001b[39m _wrap_callback(callback)\n\u001b[1;32m-> 1359\u001b[0m res \u001b[38;5;241m=\u001b[39m _minimize_bfgs(f, x0, args, fprime, callback\u001b[38;5;241m=\u001b[39mcallback, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mopts)\n\u001b[0;32m   1361\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m full_output:\n\u001b[0;32m   1362\u001b[0m     retlist \u001b[38;5;241m=\u001b[39m (res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m], res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfun\u001b[39m\u001b[38;5;124m'\u001b[39m], res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjac\u001b[39m\u001b[38;5;124m'\u001b[39m], res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhess_inv\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   1363\u001b[0m                res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnfev\u001b[39m\u001b[38;5;124m'\u001b[39m], res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnjev\u001b[39m\u001b[38;5;124m'\u001b[39m], res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_optimize.py:1418\u001b[0m, in \u001b[0;36m_minimize_bfgs\u001b[1;34m(fun, x0, args, jac, callback, gtol, norm, eps, maxiter, disp, return_all, finite_diff_rel_step, xrtol, **unknown_options)\u001b[0m\n\u001b[0;32m   1415\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m maxiter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1416\u001b[0m     maxiter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(x0) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m200\u001b[39m\n\u001b[1;32m-> 1418\u001b[0m sf \u001b[38;5;241m=\u001b[39m _prepare_scalar_function(fun, x0, jac, args\u001b[38;5;241m=\u001b[39margs, epsilon\u001b[38;5;241m=\u001b[39meps,\n\u001b[0;32m   1419\u001b[0m                               finite_diff_rel_step\u001b[38;5;241m=\u001b[39mfinite_diff_rel_step)\n\u001b[0;32m   1421\u001b[0m f \u001b[38;5;241m=\u001b[39m sf\u001b[38;5;241m.\u001b[39mfun\n\u001b[0;32m   1422\u001b[0m myfprime \u001b[38;5;241m=\u001b[39m sf\u001b[38;5;241m.\u001b[39mgrad\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_optimize.py:383\u001b[0m, in \u001b[0;36m_prepare_scalar_function\u001b[1;34m(fun, x0, jac, args, bounds, epsilon, finite_diff_rel_step, hess)\u001b[0m\n\u001b[0;32m    379\u001b[0m     bounds \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf, np\u001b[38;5;241m.\u001b[39minf)\n\u001b[0;32m    381\u001b[0m \u001b[38;5;66;03m# ScalarFunction caches. Reuse of fun(x) during grad\u001b[39;00m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;66;03m# calculation reduces overall function evaluations.\u001b[39;00m\n\u001b[1;32m--> 383\u001b[0m sf \u001b[38;5;241m=\u001b[39m ScalarFunction(fun, x0, args, grad, hess,\n\u001b[0;32m    384\u001b[0m                     finite_diff_rel_step, bounds, epsilon\u001b[38;5;241m=\u001b[39mepsilon)\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sf\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:158\u001b[0m, in \u001b[0;36mScalarFunction.__init__\u001b[1;34m(self, fun, x0, args, grad, hess, finite_diff_rel_step, finite_diff_bounds, epsilon)\u001b[0m\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m fun_wrapped(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx)\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun_impl \u001b[38;5;241m=\u001b[39m update_fun\n\u001b[1;32m--> 158\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun()\n\u001b[0;32m    160\u001b[0m \u001b[38;5;66;03m# Gradient evaluation\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(grad):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:251\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated:\n\u001b[1;32m--> 251\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun_impl()\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:155\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[1;34m()\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_fun\u001b[39m():\n\u001b[1;32m--> 155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m fun_wrapped(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:137\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnfev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m fx \u001b[38;5;241m=\u001b[39m fun(np\u001b[38;5;241m.\u001b[39mcopy(x), \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    138\u001b[0m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\model.py:534\u001b[0m, in \u001b[0;36mLikelihoodModel.fit.<locals>.f\u001b[1;34m(params, *args)\u001b[0m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf\u001b[39m(params, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m--> 534\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloglike(params, \u001b[38;5;241m*\u001b[39margs) \u001b[38;5;241m/\u001b[39m nobs\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\model.py:299\u001b[0m, in \u001b[0;36mLikelihoodModel.loglike\u001b[1;34m(self, params)\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloglike\u001b[39m(\u001b[38;5;28mself\u001b[39m, params):\n\u001b[0;32m    287\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;124;03m    Log-likelihood of model.\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;124;03m    Must be overridden by subclasses.\u001b[39;00m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 299\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from statsmodels.discrete.discrete_model import DiscreteModel\n",
    "\n",
    "# Load the data into a pandas DataFrame\n",
    "data = pd.read_csv('DriverData1.csv')\n",
    "\n",
    "# Specify the dependent and independent variables\n",
    "y = data['Choice']\n",
    "X = data[['Gap', 'RelativeSpeed', 'TW']]\n",
    "\n",
    "# Create the multinomial logit model\n",
    "logit_model = DiscreteModel(y, X, nev_factor=3)\n",
    "\n",
    "# Fit the model\n",
    "logit_results = logit_model.fit(method='bfgs', maxiter=1000, disp=True)\n",
    "\n",
    "# Print the summary\n",
    "print(logit_results.summary())\n",
    "\n",
    "# Calculate chi-squared, p-value, rho-squared, and t-statistics\n",
    "chi_squared = logit_results.llr  # Likelihood ratio chi-squared test\n",
    "p_value = logit_results.llr_pvalue  # p-value of the chi-squared test\n",
    "rho_squared = logit_results.prsquared  # McFadden's pseudo-R-squared\n",
    "t_stats = logit_results.tvalues  # t-statistics for the coefficients\n",
    "\n",
    "print(f\"Chi-squared: {chi_squared:.4f}\")\n",
    "print(f\"P-value: {p_value:.4f}\")\n",
    "print(f\"Rho-squared: {rho_squared:.4f}\")\n",
    "print(f\"T-statistics:\\n{t_stats}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393ee60c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6774aff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
